# core/memory.py â€” Ù†Ø¸Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
from __future__ import annotations
import os
import sqlite3
import time
import re
import json
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
from rank_bm25 import BM25Okapi
import hashlib

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger = logging.getLogger(__name__)

DB_PATH = os.environ.get("BASSAM_DB", "bassam_v2.db")

# ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
_AR_STOP = set("""
Ùˆ ÙÙŠ Ø¹Ù„Ù‰ Ù…Ù† Ù…Ø¹ Ø¹Ù† Ø§Ù„Ù‰ Ø¥Ù„Ù‰ Ø£Ùˆ Ø£Ù… Ø«Ù… Ø¨Ù„ Ù‚Ø¯ Ù„Ù‚Ø¯ ÙƒØ§Ù† ÙƒØ§Ù†Øª ÙƒØ§Ù†ÙˆØ§ ÙƒÙ† ÙƒÙ†Øª ÙƒÙ†ØªÙ† ÙŠÙƒÙˆÙ† ØªÙƒÙˆÙ† ØªÙƒÙˆÙ†ÙˆÙ† ØªÙƒÙˆÙ†Ù†
Ù‡Ø°Ø§ Ù‡Ø°Ù‡ Ø°Ù„Ùƒ ØªÙ„Ùƒ Ù‡Ù†Ø§Ùƒ Ù‡Ù†Ø§ Ø­ÙŠØ« ÙƒØ°Ù„Ùƒ ÙƒÙ…Ø§ ÙƒÙŠÙ Ù„Ù…Ø§Ø°Ø§ Ù„Ù…Ø§ Ø¥Ù† Ø£Ù† Ø¥Ù†Ù…Ø§ Ø£Ù†Ù…Ø§ Ø£Ù„Ø§ Ø¥Ù„Ø§ Ù…Ø§ Ù„Ø§ Ù„Ù… Ù„Ù† Ù‡Ù„
Ø¬Ø¯Ø§ Ø¬Ø¯Ø§Ù‹ ÙÙ‚Ø· Ø£ÙŠØ¶Ø§ Ø£ÙŠØ¶Ù‹Ø§ Ø£ÙŠ Ø£ÙŠÙ‹Ù‘Ø§ Ø£ÙŠÙ‡Ø§ Ø§Ù„ØªÙŠ Ø§Ù„Ø°ÙŠ Ø§Ù„Ø°ÙŠÙ† Ø§Ù„Ù„Ø°Ø§Ù† Ø§Ù„Ù„ØªØ§Ù† Ø§Ù„Ù„ÙˆØ§ØªÙŠ Ø§Ù„Ù„Ø§Ø¦ÙŠ Ø£ÙˆÙ„Ø¦Ùƒ Ù‡Ø¤Ù„Ø§Ø¡
Ø­ÙŠÙ† Ø¹Ù†Ø¯Ù…Ø§ Ø¨ÙŠÙ†Ù…Ø§ Ø£Ø«Ù†Ø§Ø¡ Ø¨Ø³Ø¨Ø¨ Ù„Ø¯Ù‰ Ø¶Ù…Ù† Ø¯ÙˆÙ† ØºÙŠØ± Ø¨ÙŠÙ† Ù‚Ø¨Ù„ Ø¨Ø¹Ø¯ Ù…Ù†Ø° Ø­ØªÙ‰ Ø®Ù„Ø§Ù„ ÙÙˆÙ‚ ØªØ­Øª Ø£Ù…Ø§Ù… ÙˆØ±Ø§Ø¡
Ù„Ø°Ù„Ùƒ Ù„Ù‡Ø°Ø§ Ù„Ø°Ø§ Ù„Ø£Ù† ÙƒÙŠ Ù„ÙƒÙŠ Ø¥Ø° Ø¥Ø°Ø§ Ø¥Ù„Ø§ Ø¥Ù†Ù‘ Ø£Ù†Ù‘ Ø£Ù„Ø§ Ù„ÙƒÙ† Ø§Ù„Ù„ÙŠ Ø§Ù„ÙŠ Ø¹Ù†Ø¯ Ø¹Ù†Ø§ Ø¹Ù†Ù‡ Ø¹Ù„ÙŠÙ‡Ø§ Ø¹Ù„ÙŠÙ‡
Ø¨Ø¹Ø¶ ÙƒÙ„ Ø§ÙŠ Ø§Ù‰ Ø¨Ø¹Ø¯ Ù‚Ø¨Ù„ Ø­ÙŠÙ† Ø¯ÙˆÙ† ØºÙŠØ± Ø³ÙˆÙ‰ Ø§Ù„Ø§ Ø¥Ù„Ø§ Ø¨Ù„Ø§ ÙÙ„Ø§Ù† Ø§Ù†Ø§ Ø§Ù†Øª Ø§Ù†ØªÙ… Ø§Ù†ØªÙ† Ù†Ø­Ù†
""".split())

# Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
_memory_cache: List[Dict] = []
_bm25: Optional[BM25Okapi] = None
_last_rebuild: float = 0
_cache_ttl: int = 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚

class MemoryManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.connection_pool = []
        self.max_connections = 5
        
    def get_connection(self) -> sqlite3.Connection:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§ØªØµØ§Ù„ Ù…Ù† Ø§Ù„Ù…Ø¬Ù…Ø¹"""
        if self.connection_pool:
            return self.connection_pool.pop()
        else:
            conn = sqlite3.connect(DB_PATH, check_same_thread=False)
            conn.row_factory = sqlite3.Row
            return conn
            
    def return_connection(self, conn: sqlite3.Connection):
        """Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…Ø¹"""
        if len(self.connection_pool) < self.max_connections:
            self.connection_pool.append(conn)
        else:
            conn.close()

# Ù…Ø¯ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ
memory_manager = MemoryManager()

def _connect() -> sqlite3.Connection:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª"""
    return memory_manager.get_connection()

def _close_connection(conn: sqlite3.Connection):
    """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„"""
    memory_manager.return_connection(conn)

def init_db():
    """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""
    conn = _connect()
    cur = conn.cursor()
    
    try:
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­Ø³Ù†
        cur.execute("""
            CREATE TABLE IF NOT EXISTS facts(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                normalized_text TEXT,
                source TEXT,
                category TEXT,
                quality_score REAL DEFAULT 1.0,
                usage_count INTEGER DEFAULT 0,
                last_used INTEGER DEFAULT 0,
                added_at INTEGER,
                hash TEXT UNIQUE
            )
        """)
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†
        cur.execute("""
            CREATE TABLE IF NOT EXISTS conversations(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_msg TEXT NOT NULL,
                bot_msg TEXT NOT NULL,
                intent_type TEXT,
                confidence REAL,
                sources_json TEXT,
                session_id TEXT,
                ts INTEGER
            )
        """)
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„ÙØ¦Ø§Øª
        cur.execute("""
            CREATE TABLE IF NOT EXISTS categories(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                description TEXT
            )
        """)
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        cur.execute("""
            CREATE TABLE IF NOT EXISTS statistics(
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                key TEXT UNIQUE,
                value INTEGER,
                updated_at INTEGER
            )
        """)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        default_categories = [
            ("Ø¨Ø±Ù…Ø¬Ø©", "Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±"),
            ("ØªÙ‚Ù†ÙŠØ©", "Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"),
            ("Ø¹Ù„Ù…", "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ©"),
            ("ØµØ­Ø©", "Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ØµØ­ÙŠØ©"),
            ("Ø¹Ø§Ù…", "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©")
        ]
        
        for name, desc in default_categories:
            cur.execute(
                "INSERT OR IGNORE INTO categories (name, description) VALUES (?, ?)",
                (name, desc)
            )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø§Ù„Ø¨Ø­Ø«
        cur.execute("CREATE INDEX IF NOT EXISTS idx_facts_text ON facts(normalized_text)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_facts_category ON facts(category)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_facts_quality ON facts(quality_score)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_conv_ts ON conversations(ts)")
        
        conn.commit()
        logger.info("âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù‡ÙŠØ£Ø© Ø¨Ù†Ø¬Ø§Ø­")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        conn.rollback()
    finally:
        _close_connection(conn)
    
    _rebuild_index()

def _enhanced_normalize(s: str) -> str:
    """ØªÙ†Ù‚ÙŠØ© Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø©"""
    if not s:
        return ""
        
    s = str(s).strip().lower()
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ØªØ¨Ø§ÙŠÙ†Ø©
    repl = {
        "Ø£": "Ø§", "Ø¥": "Ø§", "Ø¢": "Ø§", "Ø©": "Ù‡", "Ù‰": "ÙŠ", "Ù€": "",
        "Ù": "", "Ù": "", "Ù": "", "Ù‘": "", "Ù’": "", "Ù‹": "", "ÙŒ": "", "Ù": ""
    }
    for k, v in repl.items():
        s = s.replace(k, v)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„Ø±Ù…ÙˆØ²
    s = re.sub(r'[^\w\s]', ' ', s)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ù†ÙØ±Ø¯Ø©
    s = re.sub(r'\b\d+\b', ' ', s)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
    words = [w for w in s.split() if w not in _AR_STOP and len(w) > 1]
    
    return " ".join(words)

def _calculate_text_hash(text: str) -> str:
    """Ø­Ø³Ø§Ø¨ Ø¨ØµÙ…Ø© Ø§Ù„Ù†Øµ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
    normalized = _enhanced_normalize(text)
    return hashlib.md5(normalized.encode('utf-8')).hexdigest()

def _rebuild_index(force: bool = False):
    """Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
    global _memory_cache, _bm25, _last_rebuild
    
    current_time = time.time()
    if not force and current_time - _last_rebuild < _cache_ttl:
        return
        
    try:
        conn = _connect()
        cur = conn.cursor()
        
        # Ø¬Ù„Ø¨ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ù…Ø¹ Ø§Ù„ØªØµÙÙŠØ© Ø¨Ø§Ù„Ø¬ÙˆØ¯Ø©
        cur.execute("""
            SELECT id, text, source, category, quality_score 
            FROM facts 
            WHERE quality_score >= 0.3 
            ORDER BY quality_score DESC, usage_count DESC 
            LIMIT 10000
        """)
        
        rows = cur.fetchall()
        _memory_cache = [
            {
                "id": r[0],
                "text": r[1],
                "source": r[2] or "",
                "category": r[3] or "Ø¹Ø§Ù…",
                "quality": r[4]
            }
            for r in rows
        ]
        
        _close_connection(conn)
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ BM25
        if _memory_cache:
            corpus = [_enhanced_normalize(item["text"]) for item in _memory_cache]
            tokenized_corpus = [doc.split() for doc in corpus if doc.strip()]
            if tokenized_corpus:
                _bm25 = BM25Okapi(tokenized_corpus)
            else:
                _bm25 = None
        else:
            _bm25 = None
            
        _last_rebuild = current_time
        logger.info(f"ğŸ”„ ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ ({len(_memory_cache)} Ø¹Ù†ØµØ±)")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³: {e}")

def add_fact(text: str, source: str | None = None, category: str = "Ø¹Ø§Ù…"):
    """Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙŠÙ‚Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆØ§Ù„ØªÙƒØ±Ø§Ø±"""
    if not text or len(text.strip()) < 10:
        return False
        
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©
    quality = _calculate_fact_quality(text)
    if quality < 0.2:
        return False
        
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±
    text_hash = _calculate_text_hash(text)
    
    conn = _connect()
    cur = conn.cursor()
    
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Øµ Ù…Ø³Ø¨Ù‚Ø§Ù‹
        cur.execute("SELECT id FROM facts WHERE hash = ?", (text_hash,))
        existing = cur.fetchone()
        
        if existing:
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            cur.execute(
                "UPDATE facts SET usage_count = usage_count + 1, last_used = ? WHERE id = ?",
                (int(time.time()), existing[0])
            )
            conn.commit()
            _close_connection(conn)
            return True
        
        # Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙŠÙ‚Ø© Ø¬Ø¯ÙŠØ¯Ø©
        normalized = _enhanced_normalize(text)
        cur.execute("""
            INSERT INTO facts 
            (text, normalized_text, source, category, quality_score, added_at, hash) 
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (text.strip(), normalized, source, category, quality, int(time.time()), text_hash))
        
        conn.commit()
        _close_connection(conn)
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³
        _rebuild_index(force=True)
        
        logger.info(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙŠÙ‚Ø© Ø¬Ø¯ÙŠØ¯Ø©: {text[:50]}...")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©: {e}")
        conn.rollback()
        _close_connection(conn)
        return False

def _calculate_fact_quality(text: str) -> float:
    """Ø­Ø³Ø§Ø¨ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ù…Ù† 0 Ø¥Ù„Ù‰ 1"""
    if not text or len(text) < 20:
        return 0.0
        
    # Ø¹ÙˆØ§Ù…Ù„ Ø®ÙØ¶ Ø§Ù„Ø¬ÙˆØ¯Ø©
    penalty_phrases = [
        "Ø§Ù†Ù‚Ø± Ù‡Ù†Ø§", "Ø§Ø´ØªØ±Ùƒ Ø§Ù„Ø¢Ù†", "Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„",
        "Ø¥Ø¹Ù„Ø§Ù†", "Ø±Ø§Ø¨Ø· Ø®Ø§Ø±Ø¬ÙŠ", "Ø´Ø§Ù‡Ø¯ Ø£ÙŠØ¶Ø§Ù‹", "ØªØ§Ø¨Ø¹Ù†Ø§ Ø¹Ù„Ù‰"
    ]
    
    score = 0.5  # Ø¯Ø±Ø¬Ø© Ø£Ø³Ø§Ø³ÙŠØ©
    
    # Ø¹ÙˆØ§Ù…Ù„ Ø±ÙØ¹ Ø§Ù„Ø¬ÙˆØ¯Ø©
    if len(text) > 50: score += 0.1
    if len(text.split()) > 8: score += 0.1
    if any(char.isdigit() for char in text): score += 0.1
    if "." in text: score += 0.1
    if ":" in text: score += 0.05
    
    # Ø¹ÙˆØ§Ù…Ù„ Ø®ÙØ¶ Ø§Ù„Ø¬ÙˆØ¯Ø©
    for phrase in penalty_phrases:
        if phrase in text.lower():
            score -= 0.3
            break
            
    if text.count("!") > 2: score -= 0.2
    if text.count("http") > 0: score -= 0.2
    
    return max(0.1, min(1.0, score))

def save_conv(user_msg: str, bot_msg: str, intent_type: str = None, 
              confidence: float = None, sources: List[dict] = None, session_id: str = "default"):
    """Ø­ÙØ¸ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"""
    conn = _connect()
    cur = conn.cursor()
    
    try:
        sources_json = json.dumps(sources or [])
        
        cur.execute("""
            INSERT INTO conversations 
            (user_msg, bot_msg, intent_type, confidence, sources_json, session_id, ts) 
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (user_msg, bot_msg, intent_type, confidence, sources_json, session_id, int(time.time())))
        
        conn.commit()
        
        # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        _update_statistics("total_conversations")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {e}")
        conn.rollback()
    finally:
        _close_connection(conn)

def search_memory(q: str, limit: int = 5, min_score: float = 0.1, 
                 category: str = None) -> List[Dict]:
    """Ø¨Ø­Ø« Ù…Ø­Ø³Ù† ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ø¹ ØªØµÙÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©"""
    if not _bm25 or not _memory_cache or not q.strip():
        return []
    
    # ØªØ·Ø¨ÙŠØ¹ query
    norm_q = _enhanced_normalize(q)
    if not norm_q:
        return []
    
    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BM25
        tokenized_query = norm_q.split()
        scores = _bm25.get_scores(tokenized_query)
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø§Ù„ØªØµÙÙŠØ©
        results = []
        for i, (item, score) in enumerate(zip(_memory_cache, scores)):
            if score >= min_score:
                # ØªØ·Ø¨ÙŠÙ‚ ØªØµÙÙŠØ© Ø§Ù„ÙØ¦Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø­Ø¯Ø¯Ø©
                if category and item.get("category") != category:
                    continue
                    
                results.append({
                    "id": item["id"],
                    "text": item["text"],
                    "source": item["source"],
                    "category": item.get("category", "Ø¹Ø§Ù…"),
                    "quality": item.get("quality", 1.0),
                    "score": float(score),
                    "rank": i + 1
                })
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results.sort(key=lambda x: x["score"], reverse=True)
        
        # ØªØ­Ø¯ÙŠØ« Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£ÙˆÙ„Ù‰
        if results:
            _update_usage_counts([r["id"] for r in results[:3]])
        
        return results[:limit]
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
        return []

def _update_usage_counts(fact_ids: List[int]):
    """ØªØ­Ø¯ÙŠØ« Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚"""
    if not fact_ids:
        return
        
    conn = _connect()
    cur = conn.cursor()
    
    try:
        placeholders = ','.join('?' * len(fact_ids))
        cur.execute(f"""
            UPDATE facts 
            SET usage_count = usage_count + 1, last_used = ? 
            WHERE id IN ({placeholders})
        """, [int(time.time())] + fact_ids)
        
        conn.commit()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: {e}")
    finally:
        _close_connection(conn)

def _update_statistics(stat_key: str):
    """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
    conn = _connect()
    cur = conn.cursor()
    
    try:
        cur.execute("""
            INSERT INTO statistics (key, value, updated_at) 
            VALUES (?, 1, ?)
            ON CONFLICT(key) DO UPDATE SET 
            value = value + 1, updated_at = ?
        """, (stat_key, int(time.time()), int(time.time())))
        
        conn.commit()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")
    finally:
        _close_connection(conn)

def manage_memory_size(max_facts: int = 8000, max_conversations: int = 5000):
    """Ø¥Ø¯Ø§Ø±Ø© Ø­Ø¬Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ø¹ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø£Ù‡Ù…"""
    conn = _connect()
    cur = conn.cursor()
    
    try:
        # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ - Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
        cur.execute("SELECT COUNT(*) FROM facts")
        fact_count = cur.fetchone()[0]
        
        if fact_count > max_facts:
            delete_count = fact_count - max_facts
            cur.execute("""
                DELETE FROM facts 
                WHERE id IN (
                    SELECT id FROM facts 
                    ORDER BY quality_score ASC, usage_count ASC, last_used ASC 
                    LIMIT ?
                )
            """, (delete_count,))
            logger.info(f"ğŸ§¹ ØªÙ… Ø­Ø°Ù {delete_count} Ø­Ù‚ÙŠÙ‚Ø© Ù‚Ø¯ÙŠÙ…Ø©")
        
        # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª - Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø£Ø­Ø¯Ø«
        cur.execute("SELECT COUNT(*) FROM conversations")
        conv_count = cur.fetchone()[0]
        
        if conv_count > max_conversations:
            delete_count = conv_count - max_conversations
            cur.execute("""
                DELETE FROM conversations 
                WHERE id IN (
                    SELECT id FROM conversations 
                    ORDER BY ts ASC 
                    LIMIT ?
                )
            """, (delete_count,))
            logger.info(f"ğŸ§¹ ØªÙ… Ø­Ø°Ù {delete_count} Ù…Ø­Ø§Ø¯Ø«Ø© Ù‚Ø¯ÙŠÙ…Ø©")
        
        conn.commit()
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³
        _rebuild_index(force=True)
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {e}")
        conn.rollback()
    finally:
        _close_connection(conn)

def get_memory_stats() -> Dict:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    conn = _connect()
    cur = conn.cursor()
    
    try:
        cur.execute("SELECT COUNT(*) FROM facts")
        total_facts = cur.fetchone()[0]
        
        cur.execute("SELECT COUNT(*) FROM conversations")
        total_conversations = cur.fetchone()[0]
        
        cur.execute("SELECT AVG(quality_score) FROM facts")
        avg_quality = cur.fetchone()[0] or 0
        
        cur.execute("SELECT COUNT(*) FROM facts WHERE quality_score >= 0.7")
        high_quality_facts = cur.fetchone()[0]
        
        return {
            "total_facts": total_facts,
            "total_conversations": total_conversations,
            "average_quality": round(avg_quality, 2),
            "high_quality_facts": high_quality_facts,
            "cached_items": len(_memory_cache),
            "index_status": "active" if _bm25 else "inactive"
        }
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")
        return {}
    finally:
        _close_connection(conn)

def get_recent_conversations(limit: int = 10, session_id: str = "default") -> List[Dict]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©"""
    conn = _connect()
    cur = conn.cursor()
    
    try:
        cur.execute("""
            SELECT user_msg, bot_msg, intent_type, ts 
            FROM conversations 
            WHERE session_id = ? 
            ORDER BY ts DESC 
            LIMIT ?
        """, (session_id, limit))
        
        results = cur.fetchall()
        return [
            {
                "user": r[0],
                "bot": r[1],
                "intent": r[2],
                "timestamp": r[3]
            }
            for r in results
        ]
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª: {e}")
        return []
    finally:
        _close_connection(conn)

def get_context(user_msg: str, limit: int = 3) -> List[Dict]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø°ÙƒÙŠ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
    recent_conv = get_recent_conversations(limit=5)
    memory_context = search_memory(user_msg, limit=limit)
    
    return {
        "conversation_history": recent_conv[:2],
        "relevant_memory": memory_context,
        "timestamp": int(time.time())
    }
