# core/intent_analyzer.py â€” Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙˆØ§Ù„Ø°ÙƒÙŠ
from __future__ import annotations
import logging
import re
import json
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import numpy as np
from collections import defaultdict

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger = logging.getLogger(__name__)

class IntentType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
    INFORMATION_REQUEST = "information_request"
    CODE_GENERATION = "code_generation"
    PROJECT_CREATION = "project_creation"
    ANALYSIS_REQUEST = "analysis_request"
    LEARNING_REQUEST = "learning_request"
    CALCULATION = "calculation"
    EXPLANATION = "explanation"
    COMPARISON = "comparison"
    RECOMMENDATION = "recommendation"
    PROBLEM_SOLVING = "problem_solving"
    CREATIVE_TASK = "creative_task"
    SMALL_TALK = "small_talk"
    ERROR_HANDLING = "error_handling"
    UNKNOWN = "unknown"

class ComplexityLevel(Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØªØ¹Ù‚ÙŠØ¯"""
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"
    ADVANCED = "advanced"

class UrgencyLevel(Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„"""
    LOW = "low"
    NORMAL = "normal"
    HIGH = "high"
    URGENT = "urgent"

@dataclass
class IntentAnalysis:
    """Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙˆØ§ÙŠØ§"""
    primary_intent: IntentType
    confidence: float
    secondary_intents: List[Tuple[IntentType, float]]
    complexity: ComplexityLevel
    urgency: UrgencyLevel
    entities: Dict[str, Any]
    context_clues: List[str]
    suggested_actions: List[str]
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
    
    def to_dict(self) -> Dict[str, Any]:
        """ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³"""
        return {
            "primary_intent": self.primary_intent.value,
            "confidence": self.confidence,
            "secondary_intents": [(intent.value, conf) for intent, conf in self.secondary_intents],
            "complexity": self.complexity.value,
            "urgency": self.urgency.value,
            "entities": self.entities,
            "context_clues": self.context_clues,
            "suggested_actions": self.suggested_actions,
            "metadata": self.metadata
        }

class AdvancedIntentAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - ÙŠÙÙ‡Ù… Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    
    def __init__(self):
        self.pattern_library = self._build_pattern_library()
        self.entity_extractors = self._build_entity_extractors()
        self.context_analyzer = self._build_context_analyzer()
        self.confidence_calculator = self._build_confidence_calculator()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self.analysis_stats = {
            "total_analyses": 0,
            "high_confidence_analyses": 0,
            "average_confidence": 0.0,
            "intent_distribution": defaultdict(int)
        }
        
        logger.info("ğŸ¯ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")

    def _build_pattern_library(self) -> Dict[IntentType, Dict[str, Any]]:
        """Ø¨Ù†Ø§Ø¡ Ù…ÙƒØªØ¨Ø© Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†ÙˆØ§ÙŠØ§"""
        return {
            IntentType.INFORMATION_REQUEST: {
                "patterns": [
                    r"Ù…Ø§ Ù‡Ùˆ", r"Ù…Ø§Ù‡Ùˆ", r"Ù…Ø§ Ù‡ÙŠ", r"Ù…Ø§Ù‡ÙŠ", r"ÙƒÙŠÙ", r"Ù„Ù…Ø§Ø°Ø§", r"Ø£ÙŠÙ†", r"Ù…ØªÙ‰",
                    r"Ø´Ø±Ø­", r"ØªØ¹Ø±ÙŠÙ", r"Ù…ÙÙ‡ÙˆÙ…", r"Ù…Ø§ Ù…Ø¹Ù†Ù‰", r"Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ", r"Ù…Ø§ ØªÙ‚ÙˆÙ„ ÙÙŠ",
                    r"Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø¹Ù†", r"Ø£Ø±ÙŠØ¯ Ù…Ø¹Ø±ÙØ©", r"Ø§Ø¨Ø­Ø« Ø¹Ù†", r"Ù‡Ù„ ØªØ¹Ø±Ù"
                ],
                "weight": 1.0,
                "complexity_indicators": [r"Ø¨Ø§Ù„ØªÙØµÙŠÙ„", r"Ø´Ø±Ø­ Ù…ÙØµÙ„", r"ÙƒØ§Ù…Ù„", r"Ø´Ø§Ù…Ù„"]
            },
            
            IntentType.CODE_GENERATION: {
                "patterns": [
                    r"Ø§ÙƒØªØ¨ ÙƒÙˆØ¯", r"Ø§Ù†Ø´Ø¦ ÙƒÙˆØ¯", r"Ø¨Ø±Ù…Ø¬", r"Ø§ØµÙ†Ø¹ ÙƒÙˆØ¯", r"Ø¯Ø§Ù„Ø©", r"function",
                    r"Ø³ÙƒØ±ÙŠØ¨Øª", r"Ø¨Ø±Ù†Ø§Ù…Ø¬", r"ÙƒÙˆØ¯ Ù„", r"Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒÙˆØ¯", r"ÙƒÙˆØ¯ Ø¬Ø§Ù‡Ø²",
                    r"ÙƒÙˆØ¯ (?:Ø¨Ø§ÙŠØ«ÙˆÙ†|python|Ø¬Ø§ÙØ§|java|html|css|Ø¬Ø§ÙØ§Ø³ÙƒØ±ÙŠØ¨Øª|javascript)",
                    r"Ø¨Ø±Ù…Ø¬Ø©", r"ØªØ·ÙˆÙŠØ±", r"Ø§Ù†Ø´Ø§Ø¡ Ø¨Ø±Ù†Ø§Ù…Ø¬"
                ],
                "weight": 1.2,
                "complexity_indicators": [r"ÙƒØ§Ù…Ù„", r"Ù…Ø´Ø±ÙˆØ¹", r"Ù†Ø¸Ø§Ù…", r"Ù…ØªÙƒØ§Ù…Ù„", r"Ù…Ø¹Ù‚Ø¯"]
            },
            
            IntentType.PROJECT_CREATION: {
                "patterns": [
                    r"Ø§Ø¨Ù†ÙŠ Ù…Ø´Ø±ÙˆØ¹", r"Ø§Ù†Ø´Ø¦ Ù…Ø´Ø±ÙˆØ¹", r"Ø§ØµÙ†Ø¹ Ù…Ø´Ø±ÙˆØ¹", r"Ù…Ø´Ø±ÙˆØ¹ Ù„",
                    r"ØªØ·Ø¨ÙŠÙ‚", r"Ù…ÙˆÙ‚Ø¹", r"Ù†Ø¸Ø§Ù…", r"Ù…Ù†ØµØ©", r"Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…ØªÙƒØ§Ù…Ù„",
                    r"Ø§Ø¨Ù†ÙŠ Ù„ÙŠ", r"Ø§Ù†Ø´Ø¦ Ù„ÙŠ", r"ØµÙ…Ù… Ù„ÙŠ", r"Ø¹Ù…Ù„ Ù†Ø¸Ø§Ù…"
                ],
                "weight": 1.3,
                "complexity_indicators": [r"ÙƒØ¨ÙŠØ±", r"Ù…ØªÙƒØ§Ù…Ù„", r"Ø´Ø§Ù…Ù„", r"Ø§Ø­ØªØ±Ø§ÙÙŠ"]
            },
            
            IntentType.ANALYSIS_REQUEST: {
                "patterns": [
                    r"Ø­Ù„Ù„", r"Ø­Ù„Ù„ Ù„ÙŠ", r"Ù‚ÙŠÙ…", r"Ø±Ø§Ø¬Ø¹", r"Ù…Ø§ ØªÙ‚ÙŠÙŠÙ…Ùƒ", r"Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ",
                    r"ÙƒÙŠÙ ØªØ±Ù‰", r"Ù…Ø§ ØªØ­Ù„ÙŠÙ„Ùƒ", r"Ø¯Ø±Ø§Ø³Ø©", r"ØªÙ‚ÙŠÙŠÙ…", r"ÙØ­Øµ", r"ØªØ­Ù„ÙŠÙ„"
                ],
                "weight": 1.1,
                "complexity_indicators": [r"Ù…ÙØµÙ„", r"Ø´Ø§Ù…Ù„", Ø±"Ø¹Ù…ÙŠÙ‚", Ø±"Ø¯Ù‚ÙŠÙ‚"]
            },
            
            IntentType.LEARNING_REQUEST: {
                "patterns": [
                    r"ØªØ¹Ù„Ù…", r"ØªØ¹Ù„Ù… Ù…Ù†", r"Ø§Ø­ÙØ¸", r"ØªØ°ÙƒØ±", r"Ø£Ø¶Ù Ø¥Ù„Ù‰ Ø°Ø§ÙƒØ±ØªÙƒ",
                    r"ØªØ¹Ù„Ù… Ù‡Ø°Ø§", r"Ø§Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©", r"ØªØ¹Ù„Ù… ÙˆØªØ°ÙƒØ±", r"ÙƒÙ† Ø°ÙƒÙŠØ§Ù‹"
                ],
                "weight": 0.9,
                "complexity_indicators": [r"Ø¯Ø§Ø¦Ù…", Ø±"Ù…Ø³ØªÙ…Ø±", Ø±"Ø¯ÙˆØ±ÙŠ"]
            },
            
            IntentType.CALCULATION: {
                "patterns": [
                    r"Ø§Ø­Ø³Ø¨", r"Ù…Ø§ Ù…Ø¬Ù…ÙˆØ¹", r"ÙƒÙ…", r"Ø­Ù„", r"Ù…Ø¹Ø§Ø¯Ù„Ø©", r"Ø¹Ù…Ù„ÙŠØ© Ø­Ø³Ø§Ø¨ÙŠØ©",
                    r"Ù†Ø§ØªØ¬", r"Ø­Ø³Ø§Ø¨", r"Ù…Ø§ Ù†Ø§ØªØ¬", r"Ù…Ø§ Ù†ØªÙŠØ¬Ø©", r"ÙƒÙ… ÙŠØ³Ø§ÙˆÙŠ"
                ],
                "weight": 1.0,
                "complexity_indicators": [r"Ù…Ø¹Ù‚Ø¯", Ø±"ØµØ¹Ø¨", Ø±"Ù…ØªÙ‚Ø¯Ù…", Ø±"ÙƒØ¨ÙŠØ±"]
            },
            
            IntentType.EXPLANATION: {
                "patterns": [
                    r"Ø§Ø´Ø±Ø­", r"ÙˆØ¶Ø­", r"ÙØµÙ„", Ø±"Ø¨ÙŠÙ†", Ø±"Ù…Ø§ Ø§Ù„ÙØ±Ù‚", Ø±"Ù…Ø§ Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª",
                    r"ÙƒÙŠÙ ÙŠØ¹Ù…Ù„", Ø±"Ù…Ø§ Ø¢Ù„ÙŠØ©", Ø±"Ù…Ø§ Ø·Ø±ÙŠÙ‚Ø©", Ø±"Ù…Ø§ Ø®Ø·ÙˆØ§Øª"
                ],
                "weight": 1.0,
                "complexity_indicators": [r"Ù…ÙØµÙ„", Ø±"ÙˆØ§ÙÙŠ", Ø±"Ø´Ø§Ù…Ù„", Ø±"Ø¯Ù‚ÙŠÙ‚"]
            },
            
            IntentType.COMPARISON: {
                "patterns": [
                    r"Ù‚Ø§Ø±Ù†", r"Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ†", r"Ù…Ø§ Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª", r"Ø£ÙŠÙ‡Ù…Ø§ Ø£ÙØ¶Ù„",
                    r"Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ†", Ø±"Ù…Ø§ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù", Ø±"Ù…Ø§ Ø£ÙˆØ¬Ù‡ Ø§Ù„Ø´Ø¨Ù‡"
                ],
                "weight": 1.1,
                "complexity_indicators": [r"Ø´Ø§Ù…Ù„", Ø±"Ù…ÙØµÙ„", Ø±"Ø¯Ù‚ÙŠÙ‚", Ø±"ÙˆØ§ÙÙŠ"]
            },
            
            IntentType.RECOMMENDATION: {
                "patterns": [
                    r"Ù…Ø§Ø°Ø§ ØªÙ†ØµØ­", r"Ù…Ø§ ØªÙˆØµÙŠÙƒ", r"Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ", r"Ø£ÙØ¶Ù„",
                    r"Ø£Ù†ØµØ­Ù†ÙŠ", Ø±"Ù…Ø§ ØªØ±Ø´ÙŠØ­Ùƒ", Ø±"Ù…Ø§ Ø§Ù‚ØªØ±Ø§Ø­Ùƒ", Ø±"Ù…Ø§ ØªÙØ¶ÙŠÙ„Ùƒ"
                ],
                "weight": 1.0,
                "complexity_indicators": [r"Ù…ÙØµÙ„", Ø±"Ø´Ø§Ù…Ù„", Ø±"Ù…Ø¯Ø±ÙˆØ³"]
            },
            
            IntentType.PROBLEM_SOLVING: {
                "patterns": [
                    r"Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©", r"ÙƒÙŠÙ Ø£Ø­Ù„", r"Ù…Ø§ Ø§Ù„Ø­Ù„", r"ÙˆØ§Ø¬Ù‡Øª Ù…Ø´ÙƒÙ„Ø©",
                    r"Ø¹Ù†Ø¯ÙŠ issue", Ø±"Ù…Ø§ troubleshooting", Ø±"Ø¥ØµÙ„Ø§Ø­", Ø±"Ø¹Ù„Ø§Ø¬"
                ],
                "weight": 1.2,
                "complexity_indicators": [r"ØµØ¹Ø¨", Ø±"Ù…Ø¹Ù‚Ø¯", Ø±"Ù…Ø³ØªØ¹ØµÙŠ", Ø±"ÙƒØ¨ÙŠØ±"]
            },
            
            IntentType.CREATIVE_TASK: {
                "patterns": [
                    r"Ø§ØµÙ†Ø¹", r"Ø§Ø¨ØªÙƒØ±", r"Ø£Ù†Ø´Ø¦", Ø±"ØµÙ…Ù…", Ø±"Ø§ÙƒØªØ´Ù", Ø±"Ø§Ø®ØªØ±Ø¹",
                    r"ÙÙƒØ±Ø©", Ø±"Ù…Ø¨ØªÙƒØ±", Ø±"Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ", Ø±"Ø¬Ø¯ÙŠØ¯", Ø±"Ù…Ø®ØªÙ„Ù"
                ],
                "weight": 1.3,
                "complexity_indicators": [r"ÙƒØ¨ÙŠØ±", Ø±"Ù…Ø¹Ù‚Ø¯", Ø±"Ù…Ø¨ØªÙƒØ±", Ø±"ÙØ±ÙŠØ¯"]
            },
            
            IntentType.SMALL_TALK: {
                "patterns": [
                    r"Ù…Ø±Ø­Ø¨Ø§", r"Ø§Ù‡Ù„Ø§", r"ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ", r"Ù…Ù† Ø§Ù†Øª", r"Ø´ÙƒØ±Ø§", r"Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±",
                    r"ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", Ø±"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", Ø±"ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù…", Ø±"Ø­ÙŠØ§Ùƒ Ø§Ù„Ù„Ù‡"
                ],
                "weight": 0.5,
                "complexity_indicators": []
            },
            
            IntentType.ERROR_HANDLING: {
                "patterns": [
                    r"Ø®Ø·Ø£", r"error", r"Ù…Ø´ÙƒÙ„Ø©", r"Ù„Ø§ ÙŠØ¹Ù…Ù„", Ø±"Ù„Ù…Ø§Ø°Ø§ Ù„Ø§", Ø±"Ù…Ø§ Ø§Ù„Ø®Ù„Ù„",
                    r"ØªØµØ­ÙŠØ­", Ø±"Ø¥ØµÙ„Ø§Ø­", Ø±"debug", Ø±"fix", Ø±"solve"
                ],
                "weight": 1.2,
                "complexity_indicators": [r"ØµØ¹Ø¨", Ø±"Ù…Ø¹Ù‚Ø¯", Ø±"Ù…Ø³ØªØ¹ØµÙŠ"]
            }
        }

    def _build_entity_extractors(self) -> Dict[str, Any]:
        """Ø¨Ù†Ø§Ø¡ Ù…Ø³ØªØ®Ø±Ø¬ÙŠ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª"""
        return {
            "programming_languages": {
                "patterns": [
                    r"Ø¨Ø§ÙŠØ«ÙˆÙ†", r"python", r"Ø¬Ø§ÙØ§", r"java", r"Ø¬Ø§ÙØ§Ø³ÙƒØ±ÙŠØ¨Øª", r"javascript",
                    r"html", r"css", r"sql", r"Ø¨ÙŠ Ø¥ØªØ´ Ø¨ÙŠ", r"php", r"Ø±ÙˆØ¨ÙŠ", r"ruby",
                    r"Ø³ÙˆÙŠÙØª", r"swift", r"ÙƒÙˆØªÙ„Ù†", r"kotlin", r"Ø³ÙŠ Ø¨Ù„Ø³ Ø¨Ù„Ø³", r"c\+\+",
                    r"Ø³ÙŠ Ø´Ø§Ø±Ø¨", r"c#", r"ØºÙˆ", r"go", r"Ø±ÙˆØ³Øª", r"rust"
                ],
                "type": "technology"
            },
            
            "technologies": {
                "patterns": [
                    r"react", r"vue", r"angular", r"node", r"django", r"flask",
                    r"spring", r"laravel", r"express", r"fastapi", r"sqlite",
                    r"mysql", r"postgresql", r"mongodb", r"redis", r"docker"
                ],
                "type": "technology"
            },
            
            "domains": {
                "patterns": [
                    r"ÙˆÙŠØ¨", r"web", r"Ù…ÙˆØ¨Ø§ÙŠÙ„", r"mobile", r"Ø³Ø·Ø­ Ù…ÙƒØªØ¨", r"desktop",
                    r"Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", r"ai", r"ØªØ¹Ù„Ù… Ø¢Ù„Ø©", r"machine learning",
                    r"Ø¨ÙŠØ§Ù†Ø§Øª", r"data", r"Ø£Ù…Ù†", r"security", r"Ø³Ø­Ø§Ø¨ÙŠ", r"cloud"
                ],
                "type": "domain"
            },
            
            "complexity_indicators": {
                "patterns": [
                    r"Ø¨Ø³ÙŠØ·", r"Ø³Ù‡Ù„", r"Ù…Ø¨Ø¯Ø¦ÙŠ", r"Ø£ÙˆÙ„ÙŠ", r"Ù…Ø¨ØªØ¯Ø¦",
                    r"Ù…ØªÙˆØ³Ø·", r"Ø¹Ø§Ø¯ÙŠ", r"Ù…Ø¹ØªØ§Ø¯", r"Ù…Ø¹ØªØ¯Ù„",
                    r"Ù…Ø¹Ù‚Ø¯", r"ØµØ¹Ø¨", r"Ù…ØªÙ‚Ø¯Ù…", r"Ù…Ø­ØªØ±Ù", r"Ø®Ø¨ÙŠØ±",
                    r"Ø´Ø§Ù…Ù„", r"ÙƒØ§Ù…Ù„", r"Ù…ÙØµÙ„", r"ÙˆØ§ÙÙŠ", r"Ø¯Ù‚ÙŠÙ‚"
                ],
                "type": "complexity"
            },
            
            "urgency_indicators": {
                "patterns": [
                    r"Ø¹Ø§Ø¬Ù„", r"ÙÙˆØ±ÙŠ", r"Ø§Ù„Ø¢Ù†", r"Ø¨Ø³Ø±Ø¹Ø©", r"Ù…Ø³ØªØ¹Ø¬Ù„",
                    r"Ø¹Ø§Ø¯ÙŠ", r"ÙˆÙ‚Øª", r"Ù„Ø§Ø­Ù‚", r"Ù…Ø³ØªÙ‚Ø¨Ù„",
                    r"Ù…Ù‡Ù…", r"Ø¶Ø±ÙˆØ±ÙŠ", r"Ø­ÙŠÙˆÙŠ", Ø±"Ø­Ø§Ø³Ù…"
                ],
                "type": "urgency"
            }
        }

    def _build_context_analyzer(self) -> Dict[str, Any]:
        """Ø¨Ù†Ø§Ø¡ Ù…Ø­Ù„Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        return {
            "context_clues": {
                "detail_level": [r"Ø¨Ø§Ù„ØªÙØµÙŠÙ„", r"Ø¨Ø§Ø®ØªØµØ§Ø±", r"Ù…Ù„Ø®Øµ", r"Ù…ÙØµÙ„", r"Ø³Ø±ÙŠØ¹"],
                "preference": [r"Ø£ÙØ¶Ù„", r"Ù…ÙØ¶Ù„", r"Ø£Ø­Ø¨", r"Ø£Ø±ÙŠØ¯", r"Ø£ØªÙ…Ù†Ù‰"],
                "constraint": [r"Ø´Ø±Ø·", r"Ø¨Ø´Ø±Ø·", r"Ø´Ø±ÙŠØ·Ø©", r"Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡", r"Ø¨Ø¯ÙˆÙ†"],
                "example_request": [r"Ù…Ø«Ø§Ù„", r"Ù…Ø«Ù„Ø§Ù‹", r"Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„", r"ØªÙˆØ¶ÙŠØ­"]
            },
            "sentiment_indicators": {
                "positive": [r"Ø´ÙƒØ±Ø§Ù‹", r"Ù…Ù…ØªØ§Ø²", Ø±"Ø±Ø§Ø¦Ø¹", Ø±"Ø¬Ù…ÙŠÙ„", Ø±"Ø£Ø­Ø³Ù†Øª"],
                "negative": [r"Ø®Ø·Ø£", Ø±"ØºÙ„Ø·", Ø±"Ø³ÙŠØ¡", Ø±"Ù„Ø§ ÙŠØ¹Ù…Ù„", Ø±"Ù…Ø´ÙƒÙ„Ø©"],
                "confused": [r"Ù„Ù… Ø£ÙÙ‡Ù…", Ø±"Ù…Ø§Ø°Ø§", Ø±"ÙƒÙŠÙ", Ø±"Ù„Ù…Ø§Ø°Ø§", Ø±"Ø£ÙŠÙ†"]
            }
        }

    def _build_confidence_calculator(self) -> Dict[str, Any]:
        """Ø¨Ù†Ø§Ø¡ Ø­Ø§Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©"""
        return {
            "factors": {
                "pattern_match_strength": 0.3,
                "entity_presence": 0.2,
                "context_support": 0.15,
                "complexity_alignment": 0.15,
                "historical_patterns": 0.1,
                "query_structure": 0.1
            },
            "thresholds": {
                "high_confidence": 0.8,
                "medium_confidence": 0.6,
                "low_confidence": 0.4
            }
        }

    def analyze_intent(self, text: str, context: Dict[str, Any] = None) -> IntentAnalysis:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
        if context is None:
            context = {}
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
        cleaned_text = self._clean_text(text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
        entities = self._extract_entities(cleaned_text)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        intent_scores = self._calculate_intent_scores(cleaned_text, entities, context)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ø«Ø§Ù†ÙˆÙŠØ©
        primary_intent, confidence = self._select_primary_intent(intent_scores)
        secondary_intents = self._select_secondary_intents(intent_scores, primary_intent)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ ÙˆØ§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„
        complexity = self._analyze_complexity(cleaned_text, entities, primary_intent)
        urgency = self._analyze_urgency(cleaned_text, entities)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø¯Ù„Ø© Ø§Ù„Ø³ÙŠØ§Ù‚
        context_clues = self._extract_context_clues(cleaned_text)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©
        suggested_actions = self._generate_suggested_actions(primary_intent, entities, complexity)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self._update_analysis_stats(primary_intent, confidence)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
        analysis = IntentAnalysis(
            primary_intent=primary_intent,
            confidence=confidence,
            secondary_intents=secondary_intents,
            complexity=complexity,
            urgency=urgency,
            entities=entities,
            context_clues=context_clues,
            suggested_actions=suggested_actions,
            metadata={
                "text_length": len(text),
                "word_count": len(text.split()),
                "analysis_timestamp": time.time(),
                "cleaned_text": cleaned_text
            }
        )
        
        logger.info(f"ğŸ¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙˆØ§ÙŠØ§: {primary_intent.value} (Ø«Ù‚Ø©: {confidence:.2f})")
        return analysis

    def _clean_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„"""
        if not text:
            return ""
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ lowercase
        text = text.lower()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = ' '.join(text.split())
        
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ØªØ¨Ø§ÙŠÙ†Ø©
        replacements = {
            "Ø£": "Ø§", "Ø¥": "Ø§", "Ø¢": "Ø§", "Ø©": "Ù‡", "Ù‰": "ÙŠ"
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        return text

    def _extract_entities(self, text: str) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ"""
        entities = {
            "programming_languages": [],
            "technologies": [],
            "domains": [],
            "complexity_levels": [],
            "urgency_levels": [],
            "other_entities": []
        }
        
        for entity_type, config in self.entity_extractors.items():
            patterns = config["patterns"]
            entity_list = entities.get(entity_type, [])
            
            for pattern in patterns:
                matches = re.findall(pattern, text)
                entity_list.extend(matches)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            entities[entity_type] = list(set(entity_list))
        
        return entities

    def _calculate_intent_scores(self, text: str, entities: Dict[str, Any], context: Dict[str, Any]) -> Dict[IntentType, float]:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù†ÙˆØ§ÙŠØ§"""
        intent_scores = {}
        
        for intent_type, config in self.pattern_library.items():
            base_score = 0.0
            
            # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            pattern_score = self._calculate_pattern_score(text, config["patterns"])
            base_score += pattern_score * config["weight"]
            
            # Ø¯Ø¹Ù… Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
            entity_score = self._calculate_entity_score(intent_type, entities)
            base_score += entity_score * 0.2
            
            # Ø¯Ø¹Ù… Ø§Ù„Ø³ÙŠØ§Ù‚
            context_score = self._calculate_context_score(intent_type, context)
            base_score += context_score * 0.15
            
            # Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
            complexity_score = self._calculate_complexity_alignment(text, intent_type)
            base_score += complexity_score * 0.15
            
            intent_scores[intent_type] = min(base_score, 1.0)
        
        return intent_scores

    def _calculate_pattern_score(self, text: str, patterns: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·"""
        if not text:
            return 0.0
        
        total_matches = 0
        for pattern in patterns:
            if re.search(pattern, text):
                total_matches += 1
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¯Ø±Ø¬Ø©
        max_possible_matches = len(patterns)
        if max_possible_matches == 0:
            return 0.0
        
        return min(total_matches / max_possible_matches, 1.0)

    def _calculate_entity_score(self, intent_type: IntentType, entities: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø¯Ø¹Ù… Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª"""
        entity_support_map = {
            IntentType.CODE_GENERATION: ["programming_languages", "technologies"],
            IntentType.PROJECT_CREATION: ["programming_languages", "technologies", "domains"],
            IntentType.TECHNICAL_QUESTION: ["programming_languages", "technologies"],
            IntentType.ANALYSIS_REQUEST: ["domains"]
        }
        
        supported_entities = entity_support_map.get(intent_type, [])
        if not supported_entities:
            return 0.5  # Ø¯Ø±Ø¬Ø© Ù…Ø­Ø§ÙŠØ¯Ø©
        
        total_entities = 0
        matching_entities = 0
        
        for entity_type in supported_entities:
            entity_list = entities.get(entity_type, [])
            total_entities += len(entity_list)
            if entity_list:
                matching_entities += 1
        
        if total_entities == 0:
            return 0.3  # Ø¯Ø±Ø¬Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø¹Ù†Ø¯ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ÙƒÙŠØ§Ù†Ø§Øª
        
        return matching_entities / len(supported_entities)

    def _calculate_context_score(self, intent_type: IntentType, context: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø¯Ø¹Ù… Ø§Ù„Ø³ÙŠØ§Ù‚"""
        if not context:
            return 0.5
        
        # ØªØ­Ù„ÙŠÙ„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        conversation_history = context.get("conversation_history", [])
        if not conversation_history:
            return 0.5
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†ÙˆØ§ÙŠØ§ Ø³Ø§Ø¨Ù‚Ø© Ù…Ø´Ø§Ø¨Ù‡Ø©
        recent_intents = []
        for turn in conversation_history[-3:]:  # Ø¢Ø®Ø± 3 Ø£Ø¯ÙˆØ§Ø±
            user_msg = turn.get("user", "")
            if user_msg:
                # ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
                quick_analysis = self._quick_analyze(user_msg)
                recent_intents.append(quick_analysis)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        if recent_intents:
            similarity_count = sum(1 for intent in recent_intents if intent == intent_type)
            return similarity_count / len(recent_intents)
        
        return 0.5

    def _calculate_complexity_alignment(self, text: str, intent_type: IntentType) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯"""
        complexity_words = [
            "Ø¨Ø³ÙŠØ·", "Ø³Ù‡Ù„", "Ù…Ø¨Ø¯Ø¦ÙŠ", "Ù…Ø¨ØªØ¯Ø¦",
            "Ù…ØªÙˆØ³Ø·", "Ø¹Ø§Ø¯ÙŠ", "Ù…Ø¹ØªØ¯Ù„", 
            "Ù…Ø¹Ù‚Ø¯", "ØµØ¹Ø¨", "Ù…ØªÙ‚Ø¯Ù…", "Ù…Ø­ØªØ±Ù"
        ]
        
        text_lower = text.lower()
        complexity_matches = sum(1 for word in complexity_words if word in text_lower)
        
        # Ø¨Ø¹Ø¶ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ ØªØªØ·Ù„Ø¨ ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹ Ø£Ø¹Ù„Ù‰ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ
        high_complexity_intents = [
            IntentType.PROJECT_CREATION,
            IntentType.CODE_GENERATION,
            IntentType.PROBLEM_SOLVING
        ]
        
        if intent_type in high_complexity_intents:
            return min(complexity_matches * 0.3, 1.0)
        else:
            return 0.5  # Ø¯Ø±Ø¬Ø© Ù…Ø­Ø§ÙŠØ¯Ø©

    def _quick_analyze(self, text: str) -> IntentType:
        """ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù†ÙˆØ§ÙŠØ§ (Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚)"""
        cleaned_text = self._clean_text(text)
        
        for intent_type, config in self.pattern_library.items():
            for pattern in config["patterns"]:
                if re.search(pattern, cleaned_text):
                    return intent_type
        
        return IntentType.UNKNOWN

    def _select_primary_intent(self, intent_scores: Dict[IntentType, float]) -> Tuple[IntentType, float]:
        """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        if not intent_scores:
            return IntentType.UNKNOWN, 0.0
        
        primary_intent, max_score = max(intent_scores.items(), key=lambda x: x[1])
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¯Ø±Ø¬Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ù†Ø¹ØªØ¨Ø±Ù‡Ø§ unknown
        if max_score < 0.3:
            return IntentType.UNKNOWN, max_score
        
        return primary_intent, max_score

    def _select_secondary_intents(self, intent_scores: Dict[IntentType, float], primary_intent: IntentType) -> List[Tuple[IntentType, float]]:
        """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©"""
        secondary_intents = []
        
        for intent_type, score in intent_scores.items():
            if intent_type != primary_intent and score > 0.3:
                secondary_intents.append((intent_type, score))
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø©
        secondary_intents.sort(key=lambda x: x[1], reverse=True)
        
        return secondary_intents[:3]  # Ø£Ø¹Ù„Ù‰ 3 Ù†ÙˆØ§ÙŠØ§ Ø«Ø§Ù†ÙˆÙŠØ©

    def _analyze_complexity(self, text: str, entities: Dict[str, Any], primary_intent: IntentType) -> ComplexityLevel:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯"""
        complexity_indicators = {
            ComplexityLevel.SIMPLE: [r"Ø¨Ø³ÙŠØ·", r"Ø³Ù‡Ù„", r"Ù…Ø¨Ø¯Ø¦ÙŠ", r"Ø£ÙˆÙ„ÙŠ", r"Ù…Ø¨ØªØ¯Ø¦"],
            ComplexityLevel.MEDIUM: [r"Ù…ØªÙˆØ³Ø·", r"Ø¹Ø§Ø¯ÙŠ", r"Ù…Ø¹ØªØ§Ø¯", r"Ù…Ø¹ØªØ¯Ù„"],
            ComplexityLevel.COMPLEX: [r"Ù…Ø¹Ù‚Ø¯", r"ØµØ¹Ø¨", r"Ù…ØªÙ‚Ø¯Ù…", r"Ù…Ø­ØªØ±Ù"],
            ComplexityLevel.ADVANCED: [r"Ø´Ø§Ù…Ù„", r"ÙƒØ§Ù…Ù„", r"Ù…ÙØµÙ„", r"ÙˆØ§ÙÙŠ", r"Ø¯Ù‚ÙŠÙ‚"]
        }
        
        text_lower = text.lower()
        complexity_scores = {}
        
        for level, indicators in complexity_indicators.items():
            score = sum(1 for indicator in indicators if re.search(indicator, text_lower))
            complexity_scores[level] = score
        
        # Ø¨Ø¹Ø¶ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ ØªØ¹ØªØ¨Ø± Ù…Ø¹Ù‚Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ
        inherently_complex_intents = [
            IntentType.PROJECT_CREATION,
            IntentType.PROBLEM_SOLVING,
            IntentType.CREATIVE_TASK
        ]
        
        if primary_intent in inherently_complex_intents:
            complexity_scores[ComplexityLevel.COMPLEX] += 2
        
        # Ø§Ø®ØªÙŠØ§Ø± Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        max_level = max(complexity_scores.items(), key=lambda x: x[1])
        
        if max_level[1] == 0:
            # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§ØªØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…ØªÙˆØ³Ø·
            return ComplexityLevel.MEDIUM
        
        return max_level[0]

    def _analyze_urgency(self, text: str, entities: Dict[str, Any]) -> UrgencyLevel:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„"""
        urgency_indicators = {
            UrgencyLevel.LOW: [r"Ù„Ø§Ø­Ù‚", r"Ù…Ø³ØªÙ‚Ø¨Ù„", r"Ù„ÙŠØ³ Ø¹Ø§Ø¬Ù„", r"ÙÙŠ ÙˆÙ‚Øª"],
            UrgencyLevel.NORMAL: [],  # Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            UrgencyLevel.HIGH: [r"Ù…Ù‡Ù…", r"Ø¶Ø±ÙˆØ±ÙŠ", r"Ø­ÙŠÙˆÙŠ", r"Ø­Ø§Ø³Ù…"],
            UrgencyLevel.URGENT: [r"Ø¹Ø§Ø¬Ù„", r"ÙÙˆØ±ÙŠ", r"Ø§Ù„Ø¢Ù†", r"Ø¨Ø³Ø±Ø¹Ø©", r"Ù…Ø³ØªØ¹Ø¬Ù„"]
        }
        
        text_lower = text.lower()
        
        for level, indicators in urgency_indicators.items():
            for indicator in indicators:
                if re.search(indicator, text_lower):
                    return level
        
        return UrgencyLevel.NORMAL

    def _extract_context_clues(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø¯Ù„Ø© Ø§Ù„Ø³ÙŠØ§Ù‚"""
        context_clues = []
        
        for clue_type, patterns in self.context_analyzer["context_clues"].items():
            for pattern in patterns:
                if re.search(pattern, text):
                    context_clues.append(f"{clue_type}: {pattern}")
                    break
        
        for sentiment, patterns in self.context_analyzer["sentiment_indicators"].items():
            for pattern in patterns:
                if re.search(pattern, text):
                    context_clues.append(f"sentiment: {sentiment}")
                    break
        
        return context_clues

    def _generate_suggested_actions(self, primary_intent: IntentType, entities: Dict[str, Any], complexity: ComplexityLevel) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©"""
        actions = []
        
        action_templates = {
            IntentType.INFORMATION_REQUEST: [
                "Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©",
                "Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù† Ù…ØµØ§Ø¯Ø± Ø§Ù„ÙˆÙŠØ¨",
                "Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©",
                "ØªÙˆÙÙŠØ± Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø©"
            ],
            IntentType.CODE_GENERATION: [
                "ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨",
                "ØªÙˆÙÙŠØ± Ø´Ø±Ø­ Ù„Ù„ÙƒÙˆØ¯",
                "Ø¥Ø¶Ø§ÙØ© ØªØ¹Ù„ÙŠÙ‚Ø§Øª ØªÙˆØ¶ÙŠØ­ÙŠØ©",
                "Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒÙˆØ¯ generated"
            ],
            IntentType.PROJECT_CREATION: [
                "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª",
                "ØªØµÙ…ÙŠÙ… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹",
                "Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹",
                "ØªÙˆÙÙŠØ± Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°"
            ],
            IntentType.ANALYSIS_REQUEST: [
                "ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ù…Ù‚ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹",
                "ØªÙˆÙÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ¨ÙŠØ§Ù†Ø§Øª",
                "Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ù†Ø¸Ù…Ø©",
                "ØªÙ‚Ø¯ÙŠÙ… ØªÙˆØµÙŠØ§Øª Ø¹Ù…Ù„ÙŠØ©"
            ]
        }
        
        # Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¹Ø§Ù…Ø©
        general_actions = [
            "Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª",
            "ØªÙˆÙÙŠØ± Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©",
            "Ù…Ø±Ø§Ø¹Ø§Ø© Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨",
            "Ø§Ù„Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©"
        ]
        
        # Ø¥Ø¶Ø§ÙØ© Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„Ù†ÙŠØ©
        actions.extend(action_templates.get(primary_intent, []))
        
        # Ø¥Ø¶Ø§ÙØ© Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¹Ø§Ù…Ø©
        actions.extend(general_actions)
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        if complexity == ComplexityLevel.COMPLEX:
            actions.append("ØªÙˆÙÙŠØ± ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ù…Ù‚ ÙˆØ´Ø§Ù…Ù„")
        elif complexity == ComplexityLevel.SIMPLE:
            actions.append("Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø² ÙˆØ§Ù„ÙˆØ¶ÙˆØ­")
        
        return actions

    def _update_analysis_stats(self, primary_intent: IntentType, confidence: float):
        """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        self.analysis_stats["total_analyses"] += 1
        self.analysis_stats["intent_distribution"][primary_intent.value] += 1
        
        if confidence >= self.confidence_calculator["thresholds"]["high_confidence"]:
            self.analysis_stats["high_confidence_analyses"] += 1
        
        # ØªØ­Ø¯ÙŠØ« Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©
        total = self.analysis_stats["total_analyses"]
        current_avg = self.analysis_stats["average_confidence"]
        self.analysis_stats["average_confidence"] = (
            (current_avg * (total - 1) + confidence) / total
        )

    def get_analysis_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        return {
            **self.analysis_stats,
            "confidence_thresholds": self.confidence_calculator["thresholds"],
            "total_intent_types": len(IntentType)
        }

    def optimize_patterns(self, feedback_data: List[Tuple[str, IntentType, bool]]):
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        successful_matches = 0
        total_feedback = len(feedback_data)
        
        for text, expected_intent, was_correct in feedback_data:
            if was_correct:
                successful_matches += 1
        
        accuracy = successful_matches / total_feedback if total_feedback > 0 else 0.0
        
        logger.info(f"ğŸ“Š Ø¯Ù‚Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙˆØ§ÙŠØ§: {accuracy:.2f} ({successful_matches}/{total_feedback})")
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¯Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©ØŒ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù‡Ù†Ø§
        if accuracy < 0.7:
            logger.warning("âš ï¸ Ø¯Ù‚Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ù…Ù†Ø®ÙØ¶Ø© - ÙŠÙˆØµÙ‰ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·")

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø§Ù„Ù…ÙŠ
_global_intent_analyzer = None

def get_global_intent_analyzer() -> AdvancedIntentAnalyzer:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­Ù„Ù„ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ"""
    global _global_intent_analyzer
    if _global_intent_analyzer is None:
        _global_intent_analyzer = AdvancedIntentAnalyzer()
    return _global_intent_analyzer

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹
def quick_intent_analysis(text: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù†ÙˆØ§ÙŠØ§"""
    analyzer = get_global_intent_analyzer()
    analysis = analyzer.analyze_intent(text)
    return analysis.to_dict()
