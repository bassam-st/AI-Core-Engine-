# core/brain.py â€” Ø¯Ù…Ø§Øº Ù†ÙˆØ§Ø© Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø°ÙƒÙŠØ© (Ø¨Ø­Ø« + ÙˆÙŠÙƒÙŠ + ÙƒÙˆØ¯ + ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ÙƒÙˆØ¯)
from __future__ import annotations
from typing import List, Tuple
from core.memory import search_memory, add_fact, save_conv
from core.web_search import web_search, fetch_text, wiki_summary_ar

OPENERS = ["Ø£ÙƒÙŠØ¯!", "ØªÙ…Ø§Ù…ØŒ", "Ø®Ù„Ù‘ÙŠÙ†ÙŠ Ø£Ø®ØªØµØ± Ù„Ùƒ:", "Ø¨Ø§Ø®ØªØµØ§Ø±:"]

SMALL_TALK = {
    "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ": "Ø£Ù†Ø§ Ø¨Ø®ÙŠØ± ÙˆÙ„Ù„Ù‡ Ø§Ù„Ø­Ù…Ø¯ ğŸ™Œ Ø¬Ø§Ù‡Ø² Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ Ø£ÙŠ Ø´ÙŠØ¡.",
    "Ù…Ù† Ø§Ù†Øª": "Ø£Ù†Ø§ Ù†ÙˆØ§Ø© Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø°ÙƒÙŠØ© â€” Ø£Ø¨Ø­Ø« ÙˆØ£Ù„Ø®Ù‘Øµ ÙˆØ£ØªØ¹Ù„Ù‘Ù… Ù…Ù† ÙƒÙ„Ø§Ù…Ùƒ.",
    "ÙˆØ´ Ø§Ø®Ø¨Ø§Ø±Ùƒ": "ÙƒÙ„ Ø´ÙŠØ¡ ØªÙ…Ø§Ù…! Ù‚Ù„ Ù„ÙŠ Ù…Ø§Ø°Ø§ ØªØ±ÙŠØ¯ Ø£Ù† Ø£Ø¹Ù…Ù„ Ø§Ù„Ø¢Ù†ØŸ",
    "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…": "ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ØŒ Ø­ÙŠÙ‘Ø§Ùƒ Ø§Ù„Ù„Ù‡ ğŸŒŸ",
}

def _pick_opener(i: int = 0) -> str:
    return OPENERS[i % len(OPENERS)]

def _summarize_snippets(snippets: List[str], max_lines: int = 6) -> List[str]:
    sents: List[str] = []
    for t in snippets:
        if not t:
            continue
        for p in t.split("."):
            p = p.strip()
            if 20 < len(p) < 240:
                sents.append(p)
    sents = sorted(sents, key=len, reverse=True)
    seen, out = set(), []
    for s in sents:
        k = s[:40]
        if k not in seen:
            seen.add(k); out.append(s)
        if len(out) >= max_lines:
            break
    return out

def _format_sources(sources: List[dict] | None) -> List[dict]:
    safe = sources or []
    return [{"title": s.get("title",""), "url": s.get("url","")} for s in safe]

def _normalize_ar(s: str) -> str:
    return (s or "").replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§").replace("Ø©","Ù‡").lower()

def _small_talk_reply(q: str) -> str | None:
    nq = _normalize_ar(q)
    for k, v in SMALL_TALK.items():
        if _normalize_ar(k) in nq:
            return v
    return None

def _escape_html(s: str) -> str:
    return s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def _is_code_intent(q: str) -> bool:
    nq = _normalize_ar(q)
    # ÙƒÙ„Ù…Ø§Øª Ù…ØªÙ†ÙˆÙ‘Ø¹Ø© Ù„Ù„Ø·Ù„Ø¨: Ø§Ù†Ø´Ø¦/Ø§Ù†Ø´Ø§/Ø³ÙˆÙŠ/Ø§ÙƒØªØ¨/Ø§Ø¹Ø·Ù†ÙŠ/ÙˆÙ„Ø¯ ... + ÙƒÙˆØ¯/Ø§ÙƒÙˆØ§Ø¯/Ø¨Ø±Ù…Ø¬Ù‡/Ø´ÙØ±Ù‡
    triggers_any = any(w in nq for w in ["Ø§Ù†Ø´Ø¦","Ø§Ù†Ø´Ø§","Ø³ÙˆÙŠ","Ø§ÙƒØªØ¨","Ø§Ø¹Ø·Ù†ÙŠ","ÙˆÙ„Ø¯","Ø§Ù†Ø´Ø§Ø¡"])
    code_words   = any(w in nq for w in ["ÙƒÙˆØ¯","Ø§ÙƒÙˆØ§Ø¯","Ø¨Ø±Ù…Ø¬Ù‡","Ø¨Ø±Ù…Ø¬","Ø´ÙØ±Ù‡","javascript","python","html","css","sql","Ø¬Ø§ÙØ§"])
    return ("ÙƒÙˆØ¯" in nq) or (triggers_any and code_words)

def chat_answer(q: str) -> Tuple[str, List[dict]]:
    q = (q or "").strip()
    if not q:
        return "Ø§Ø°ÙƒØ± Ø³Ø¤Ø§Ù„Ùƒ Ù…Ù† ÙØ¶Ù„Ùƒ.", []

    # 0) Ø±Ø¯ÙˆØ¯ ÙˆØ¯Ù‘ÙŠØ©
    st = _small_talk_reply(q)
    if st:
        save_conv(q, st)
        return st, []

    # 0.5) âš¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯ Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹ Ø¥Ù† ÙƒØ§Ù†Øª Ø§Ù„Ù†ÙŠØ© Ø¨Ø±Ù…Ø¬ÙŠØ© (Ù‚Ø¨Ù„ Ø£ÙŠ Ø¨Ø­Ø«/ØªÙ„Ø®ÙŠØµ)
    if _is_code_intent(q):
        from core.coder import generate_code
        result = generate_code(q)            # {title, lang, code}
        code = result["code"]; lang = result["lang"]; title = result["title"]
        code_html = _escape_html(code)
        reply = f"ğŸ”§ {title}\n\n<pre><code>{code_html}</code></pre>"
        # Ù†ØªØ¹Ù„Ù‘Ù… Ù…Ù† Ø§Ù„ÙƒÙˆØ¯
        add_fact(f"Ù…Ø«Ø§Ù„ ÙƒÙˆØ¯ ({lang}) Ø¨Ø¹Ù†ÙˆØ§Ù†: {title}.", source="codegen")
        add_fact(f"Ù…Ù‚ØªØ·Ù {lang}: {code.strip()[:1200]}", source="codegen")
        save_conv(q, reply)
        return reply, []

    # 1) Ø°Ø§ÙƒØ±Ø©
    mem_hits = search_memory(q, limit=5)
    mem_texts = [h["text"] for h in mem_hits]

    # 2) Ø¨Ø­Ø« ÙˆÙŠØ¨
    need_web = len(mem_hits) == 0 or (mem_hits and mem_hits[0]["score"] < 1.5)
    try:
        web_results = web_search(q, max_results=6) if need_web else []
    except Exception:
        web_results = []

    # 3) Ø¬Ù„Ø¨ Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· + Ù…Ù„Ø®Øµ ÙˆÙŠÙƒÙŠ
    page_texts: List[str] = []
    for r in web_results[:3]:
        u = r.get("url")
        if not u: 
            continue
        txt = fetch_text(u)
        if txt:
            page_texts.append(txt)

    if need_web and not page_texts:
        wk = wiki_summary_ar(q)
        if wk:
            page_texts.append(wk)

    # 4) ØªÙ„Ø®ÙŠØµ ÙˆØªØ±ÙƒÙŠØ¨ Ø¬ÙˆØ§Ø¨
    summary_lines = _summarize_snippets(mem_texts + page_texts, max_lines=6)
    if not summary_lines:
        reply = "Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø§Ù„Ø¢Ù†. Ø¬Ø±Ù‘Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© Ø£Ùˆ Ø¹Ù„Ù‘Ù…Ù†ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¨Ù‚ÙˆÙ„Ùƒ: Â«Ø£Ø¶Ù Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©: ...Â»."
        save_conv(q, reply)
        return reply, _format_sources(web_results[:5])

    opener = _pick_opener(len(mem_texts) + len(page_texts))
    body = ("\n- " + "\n- ".join(summary_lines)) if summary_lines else ""
    reply = f"{opener} {body.strip()}".strip()

    # 5) ØªØ¹Ù„Ù‘Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Ø§Ù„Ù…Ù„Ø®Øµ
    for line in summary_lines[:3]:
        if len(line) > 40:
            add_fact(line, source="autolearn")

    save_conv(q, reply)
    return reply, _format_sources(web_results[:5])
