# core/brain.py â€” Ø¯Ù…Ø§Øº: Ø¨Ø­Ø« + ÙˆÙŠÙƒÙŠ + ÙƒÙˆØ¯ + ÙØ±ÙŠÙ‚ Ø£ÙƒÙˆØ§Ø¯ + ØªØ¹Ù„Ù‘Ù…
from __future__ import annotations
from typing import List, Tuple
from core.memory import search_memory, add_fact, save_conv
from core.web_search import web_search, fetch_text, wiki_summary_ar
from core.code_team import build_project
from core.coder import generate_code

OPENERS = ["Ø£ÙƒÙŠØ¯!", "ØªÙ…Ø§Ù…ØŒ", "Ø®Ù„Ù‘ÙŠÙ†ÙŠ Ø£Ø®ØªØµØ± Ù„Ùƒ:", "Ø¨Ø§Ø®ØªØµØ§Ø±:"]
SMALL_TALK = {
    "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ": "Ø£Ù†Ø§ Ø¨Ø®ÙŠØ± ÙˆÙ„Ù„Ù‡ Ø§Ù„Ø­Ù…Ø¯ ğŸ™Œ Ø¬Ø§Ù‡Ø² Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ Ø£ÙŠ Ø´ÙŠØ¡.",
    "Ù…Ù† Ø§Ù†Øª": "Ø£Ù†Ø§ Ù†ÙˆØ§Ø© Ø¨Ø³Ù‘Ø§Ù… Ø§Ù„Ø°ÙƒÙŠØ© â€” Ø£Ø¨Ø­Ø« ÙˆØ£Ù„Ø®Ù‘Øµ ÙˆØ£ØªØ¹Ù„Ù‘Ù… Ù…Ù† ÙƒÙ„Ø§Ù…Ùƒ.",
    "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…": "ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ØŒ Ø­ÙŠÙ‘Ø§Ùƒ Ø§Ù„Ù„Ù‡ ğŸŒŸ",
}

def _normalize_ar(s: str) -> str:
    return (s or "").replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§").replace("Ø©","Ù‡").lower()

def _small_talk_reply(q: str) -> str | None:
    nq = _normalize_ar(q)
    for k, v in SMALL_TALK.items():
        if _normalize_ar(k) in nq: return v
    return None

def _escape_html(s: str) -> str:
    return s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def _is_code_intent(q: str) -> bool:
    nq = _normalize_ar(q)
    return any(w in nq for w in ["ÙƒÙˆØ¯","Ø§ÙƒÙˆØ§Ø¯","Ø¨Ø±Ù…Ø¬Ù‡","Ø¨Ø±Ù…Ø¬","Ø´ÙØ±Ù‡","javascript","python","html","css","sql","java","react","vue","go","rust","dart","c++","c#","php","kotlin","swift"])

def _is_project_intent(q: str) -> bool:
    nq = _normalize_ar(q)
    return any(w in nq for w in ["Ù…Ø´Ø±ÙˆØ¹","Ù…ÙˆÙ‚Ø¹","ØªØ·Ø¨ÙŠÙ‚","api","Ø®Ø§Ø¯Ù…","Ø³ÙŠØ±ÙØ±","landing","ØµÙØ­Ù‡","ØµÙØ­Ø©"])

def _summarize_snippets(snippets: List[str], max_lines: int = 6) -> List[str]:
    sents: List[str] = []
    for t in snippets or []:
        for p in (t or "").split("."):
            p = p.strip()
            if 20 < len(p) < 240: sents.append(p)
    sents = sorted(sents, key=len, reverse=True)
    out, seen = [], set()
    for s in sents:
        k = s[:40]
        if k in seen: continue
        seen.add(k); out.append(s)
        if len(out) >= max_lines: break
    return out

def _format_sources(sources: List[dict] | None) -> List[dict]:
    return [{"title": s.get("title",""), "url": s.get("url","")} for s in (sources or [])]

def chat_answer(q: str) -> Tuple[str, List[dict]]:
    q = (q or "").strip()
    if not q: return "Ø§Ø°ÙƒØ± Ø³Ø¤Ø§Ù„Ùƒ Ù…Ù† ÙØ¶Ù„Ùƒ.", []

    # 0) Ø­ÙˆØ§Ø± ÙˆØ¯Ù‘ÙŠ
    st = _small_talk_reply(q)
    if st: save_conv(q, st); return st, []

    # 0.5) Ù†ÙŠØ© Ù…Ø´Ø±ÙˆØ¹/ÙƒÙˆØ¯ â€” Ù‚Ø¨Ù„ Ø£ÙŠ Ø¨Ø­Ø«
    if _is_project_intent(q):
        proj = build_project(q)
        if proj.get("ok"):
            files = proj.get("files", {})
            names = list(files.keys())
            show = ""
            if names:
                first = names[0]
                show = f"<pre><code>{_escape_html(files[first])}</code></pre>"
            reply = "ğŸ§© ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø´Ø±ÙˆØ¹ Ø£ÙˆÙ‘Ù„ÙŠ.\n" \
                    f"- Ø§Ù„Ù…Ù„ÙØ§Øª: {', '.join(names) or 'Ù„Ø§ ÙŠÙˆØ¬Ø¯'}\n" \
                    f"- Ù…Ù„Ø§Ø­Ø¸Ø§Øª: {len(proj.get('issues', []))}\n" \
                    f"- Ù†ØµÙŠØ­Ø©: {proj.get('tips','')}\n\n" + show
            save_conv(q, reply)
            # Ù†ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
            for n in names[:3]:
                add_fact(f"Ù…Ù„Ù Ù…Ø´Ø±ÙˆØ¹: {n} ({len(files[n])} chars).", source="code-team")
            return reply, []

    if _is_code_intent(q):
        result = generate_code(q)
        code, lang, title = result["code"], result["lang"], result["title"]
        reply = f"ğŸ”§ {title}\n\n<pre><code>{_escape_html(code)}</code></pre>"
        add_fact(f"Ù…Ø«Ø§Ù„ ÙƒÙˆØ¯ {lang}: {title}.", source="codegen")
        add_fact(f"Ù…Ù‚ØªØ·Ù {lang}: {code.strip()[:800]}", source="codegen")
        save_conv(q, reply)
        return reply, []

    # 1) Ø°Ø§ÙƒØ±Ø©
    mem_hits = search_memory(q, limit=5)
    mem_texts = [h["text"] for h in mem_hits]

    # 2) Ø¨Ø­Ø« ÙˆÙŠØ¨ (DuckDuckGo + ÙˆÙŠÙƒÙŠ fallback)
    need_web = not mem_hits or (mem_hits and mem_hits[0]["score"] < 1.5)
    try: web_results = web_search(q, max_results=6) if need_web else []
    except Exception: web_results = []
    page_texts: List[str] = []
    for r in web_results[:3]:
        u = r.get("url"); 
        if not u: continue
        txt = fetch_text(u)
        if txt: page_texts.append(txt)
    if need_web and not page_texts:
        wk = wiki_summary_ar(q)
        if wk: page_texts.append(wk)

    # 3) Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø®ØªØµØ±Ø©
    summary = _summarize_snippets(mem_texts + page_texts, max_lines=6)
    if not summary:
        msg = "Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø§Ù„Ø¢Ù†. Ø¬Ø±Ù‘Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© Ø£Ùˆ Ø§Ø·Ù„Ø¨ Ù…Ù†ÙŠ Ù…Ø´Ø±ÙˆØ¹/ÙƒÙˆØ¯."
        save_conv(q, msg); return msg, _format_sources(web_results[:5])

    opener = OPENERS[(len(mem_texts)+len(page_texts)) % len(OPENERS)]
    reply = f"{opener}\n- " + "\n- ".join(summary)
    for line in summary[:3]:
        if len(line) > 40: add_fact(line, source="autolearn")
    save_conv(q, reply)
    return reply, _format_sources(web_results[:5])
