# engine/generator.py
from __future__ import annotations
from typing import List, Tuple
import re
from urllib.parse import urlparse


class AnswerSynthesizer:
    """
    Ù…ÙˆÙ„Ù‘ÙØ¯ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¨Ø³ÙŠØ· Ø¨Ø¯ÙˆÙ† Ù†Ù…Ø§Ø°Ø¬ Ø¶Ø®Ù…Ø©:
    - ÙŠÙƒÙˆÙ‘Ù† Ø®Ù„Ø§ØµØ© Ù‚ØµÙŠØ±Ø© Ù…Ù† (ÙˆÙŠÙƒÙŠ -> Ø§Ù„ÙˆÙŠØ¨ -> Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ù„ÙŠ)
    - ÙŠÙ†Ø³Ù‘Ù‚ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    - ÙŠÙ†Ø¸Ù‘Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø·ÙˆÙŠÙ„Ø© ÙˆÙŠØ¹Ø±Ø¶ Ù‚Ø§Ø¦Ù…Ø© Ù…ØµØ§Ø¯Ø± Ù…Ø®ØªØµØ±Ø©
    - ÙŠØ¯Ø¹Ù… Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„ÙƒØªØ§Ø¨Ø©: ÙˆØ¯ÙˆØ¯/Ø±Ø³Ù…ÙŠ/Ù…Ø®ØªØµØ±
    """

    def __init__(self):
        self.style = "friendly"

    # ---------- ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ----------
    def set_style(self, mode: str):
        if mode in ("friendly", "formal", "brief"):
            self.style = mode

    def _style_wrap(self, text: str) -> str:
        if self.style == "formal":
            return "Ø¥Ù„ÙŠÙƒ Ø®Ù„Ø§ØµØ© Ù…Ù†Ø¸Ù‘ÙÙ…Ø©:\n\n" + text
        if self.style == "brief":
            return text
        return "Ø¨ÙƒÙ„ ÙˆØ¯ØŒ Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\n\n" + text

    # ---------- Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© ----------
    _URL_RE = re.compile(r"https?://\S+")
    _BOX_URL_RE = re.compile(r"\[(https?://[^\]]+)\]")  # Ù†Ù…Ø· [https://...]
    _WS_RE = re.compile(r"[ \t]+")
    _NL_RE = re.compile(r"\n{3,}")

    def _clean_text(self, s: str) -> str:
        # Ù†Ø­Ø°Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø®Ø§Ù… Ø§Ù„Ø·ÙˆÙŠÙ„Ø© ÙˆÙ†Ø±ØªÙ‘Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
        s = self._URL_RE.sub("", s)
        s = self._WS_RE.sub(" ", s)
        s = self._NL_RE.sub("\n\n", s)
        return s.strip()

    def _domain(self, url: str) -> str:
        try:
            netloc = urlparse(url).netloc
            return netloc.lstrip("www.")
        except Exception:
            return url

    def _extract_urls_from_snippets(self, web_snippets: List[str]) -> List[str]:
        urls: List[str] = []
        for sn in web_snippets or []:
            # Ù„Ø¯ÙŠÙ†Ø§ ØªÙ†Ø³ÙŠÙ‚ "- Ù…Ù‚Ø·Ø¹ â€¦ [url]" Ù…Ù† Ù…Ø³Ø§Ø± /chat
            m = self._BOX_URL_RE.search(sn)
            if m:
                urls.append(m.group(1))
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
        seen = set()
        uniq = []
        for u in urls:
            if u not in seen:
                seen.add(u)
                uniq.append(u)
        return uniq

    def _pick_short_answer(self, wiki: str, web_snippets: List[str], context: str) -> str:
        # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©: ÙˆÙŠÙƒÙŠ â†’ Ø£ÙˆÙ„ Ø³Ø·Ø± ÙˆØ§Ø¶Ø­ Ù…Ù† Ø§Ù„ÙˆÙŠØ¨ â†’ Ø¬Ø²Ø¡ Ù…ÙˆØ¬Ø² Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ù„ÙŠ
        # 1) Ù…Ù† ÙˆÙŠÙƒÙŠ
        if wiki:
            w = self._clean_text(wiki)
            if w:
                return (w[:400] + "â€¦") if len(w) > 430 else w

        # 2) Ù…Ù† Ø§Ù„ÙˆÙŠØ¨ (Ù†Ø­Ø§ÙˆÙ„ Ø£Ø®Ø° Ø£ÙˆÙ„ Ø³Ø·Ø± Ù‚Ø¨Ù„ Ø£ÙŠ Ø£Ù‚ÙˆØ§Ø³/Ø±ÙˆØ§Ø¨Ø·)
        for sn in web_snippets or []:
            # Ø§Ø­Ø°Ù Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø© "- "
            s = sn.lstrip("- ").strip()
            # Ø§ÙØµÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø·
            s = s.split("[http", 1)[0].strip()
            s = self._clean_text(s)
            if len(s) >= 40:  # Ø³Ø·Ø± Ù…ÙÙ‡ÙˆÙ…
                return (s[:400] + "â€¦") if len(s) > 430 else s

        # 3) Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ù„ÙŠ
        if context:
            c = self._clean_text(context)
            return (c[:400] + "â€¦") if len(c) > 430 else c

        return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ù…ÙˆØ«ÙˆÙ‚."

    # ---------- Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ----------
    def answer(
        self,
        *,
        query: str,
        context: str,
        intent: str,
        sentiment: str,
        web_snippets: List[str],
        wiki: str,
    ) -> str:
        """
        ØªÙˆÙ„ÙŠÙ Ø¥Ø¬Ø§Ø¨Ø© Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø±ØªØ¨Ø©:
        - â“ Ø§Ù„Ø³Ø¤Ø§Ù„
        - ğŸ“˜ Ø®Ù„Ø§ØµØ© Ù‚ØµÙŠØ±Ø© (Ù…Ù† ÙˆÙŠÙƒÙŠ/Ø§Ù„ÙˆÙŠØ¨/Ø§Ù„Ø³ÙŠØ§Ù‚)
        - ğŸ“Œ Ø£Ø¯Ù„Ø© Ù…Ø®ØªØµØ±Ø© (Ø­ØªÙ‰ 3 Ø¹Ù†Ø§ØµØ±)
        - ğŸ”— Ù…ØµØ§Ø¯Ø± Ø¨Ø±ÙˆØ§Ø¨Ø· Ù†Ø¸ÙŠÙØ© (Ø­ØªÙ‰ 5)
        - ğŸ§  Ø§Ù„Ù†ÙŠØ©/Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        """
        web_snippets = web_snippets or []

        # 1) Ø®Ù„Ø§ØµØ© Ù‚ØµÙŠØ±Ø©
        short_answer = self._pick_short_answer(wiki, web_snippets, context)

        # 2) Ø£Ø¯Ù„Ø© Ù…Ø®ØªØµØ±Ø© (Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ 2-3 Ø£Ø³Ø·Ø± Ù…ÙˆØ¬Ø²Ø© Ù…Ù† Ø§Ù„ÙˆÙŠØ¨/Ø§Ù„Ø³ÙŠØ§Ù‚)
        evidence: List[str] = []
        for sn in web_snippets[:3]:
            s = sn.lstrip("- ").split("[http", 1)[0].strip()
            s = self._clean_text(s)
            if s:
                evidence.append(s[:180] + ("â€¦" if len(s) > 200 else ""))
        if not evidence and context:
            c_first = self._clean_text(context)[:180]
            if c_first:
                evidence.append(c_first + ("â€¦" if len(c_first) >= 180 else ""))

        # 3) Ù…ØµØ§Ø¯Ø± Ù†Ø¸ÙŠÙØ©
        urls = self._extract_urls_from_snippets(web_snippets)[:5]
        sources_line = ""
        if urls:
            domains = [self._domain(u) for u in urls]
            sources_line = " â€” Ø§Ù„Ù…ØµØ§Ø¯Ø±: " + " | ".join(domains)

        # 4) Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        body_parts: List[str] = [
            f"â“ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}",
            "ğŸ“˜ Ø§Ù„Ø®Ù„Ø§ØµØ©:\n" + short_answer,
        ]
        if evidence:
            ev_str = "\n".join(f"- {e}" for e in evidence)
            body_parts.append("ğŸ“Œ Ø£Ø¯Ù„Ø© Ù…Ø®ØªØµØ±Ø©:\n" + ev_str)

        meta = f"ğŸ§  Ø§Ù„Ù†ÙŠØ©: {intent} â€” Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {sentiment}"
        if sources_line:
            meta += sources_line

        body_parts.append(meta)

        final_text = "\n\n".join(body_parts)
        final_text = self._clean_text(final_text)

        return self._style_wrap(final_text)
