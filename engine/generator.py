# engine/generator.py
from __future__ import annotations
from typing import List, Optional, Tuple
import re
from urllib.parse import urlparse

# Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ scikit-learn (Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù†Ø¯Ùƒ ÙÙŠ requirements)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


class AnswerSynthesizer:
    """
    Ù…ÙˆÙ„Ù‘Ø¯ Ù…Ø­Ù„ÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ (Ø¨Ø¯ÙˆÙ† LLM):
    - ÙŠØ¯Ù…Ø¬ ÙˆÙŠÙƒÙŠ + Ø§Ù„ÙˆÙŠØ¨ + Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ù„ÙŠ.
    - ÙŠØ¬Ø²Ù‘Ø¦ Ø¥Ù„Ù‰ Ø¬Ù…Ù„ØŒ ÙˆÙŠØ­Ø³Ø¨ ØµÙ„Ø© ÙƒÙ„ Ø¬Ù…Ù„Ø© Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ø¨Ø± TF-IDF.
    - ÙŠØ²ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø± (MMR Ø¨Ø³ÙŠØ·) ÙˆÙŠØ¨Ù†ÙŠ Ø¬ÙˆØ§Ø¨Ù‹Ø§ Ù…Ù†Ø¸Ù‘Ù…Ù‹Ø§ Ø¨Ø£Ø³Ø§Ù„ÙŠØ¨ Ù…Ø®ØªÙ„ÙØ©.
    - ÙŠØ¶ÙŠÙ Ù‚Ø³Ù… "ğŸ›  Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ©" ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯ Ø£ÙØ¹Ø§Ù„ Ø£Ù…Ø±/ØªÙ†ÙÙŠØ°.
    """

    def __init__(self):
        self.style = "friendly"  # friendly | formal | brief

    # ---------- Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ÙƒØªØ§Ø¨Ø© ----------
    def set_style(self, mode: str):
        if mode in ("friendly", "formal", "brief"):
            self.style = mode

    def _wrap_style(self, text: str) -> str:
        if self.style == "formal":
            return "Ø¥Ù„ÙŠÙƒ Ø¹Ø±Ø¶Ù‹Ø§ Ù…Ù†Ø¸Ù‘Ù…Ù‹Ø§:\n\n" + text
        if self.style == "brief":
            return text
        return "Ø¨ÙƒÙ„ ÙˆØ¯ØŒ Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\n\n" + text

    # ---------- Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© ----------
    _URL_RE = re.compile(r"https?://\S+")
    _BOX_URL_RE = re.compile(r"\[(https?://[^\]]+)\]")  # Ù†Ù…Ø· [https://...]
    _WS_RE = re.compile(r"[ \t]+")
    _NL_RE = re.compile(r"\n{3,}")
    _SPLIT_RE = re.compile(r"(?<=[\.!\ØŸ\?])\s+|\n+")  # ØªÙ‚Ø³ÙŠÙ… Ø¬Ù…Ù„ Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ù…Ø¨Ø³Ù‘Ø·

    # ØªØ¨Ø³ÙŠØ· Ø¹Ø±Ø¨ÙŠ Ø¨Ø³ÙŠØ· (Ø¨Ø¯ÙˆÙ† Ø­Ø±ÙƒØ§Øª)
    _DIAC_RE = re.compile(r"[\u064B-\u065F\u0670]")  # Ø§Ù„ØªØ´ÙƒÙŠÙ„
    _ALEF_RE = re.compile(r"[Ø¥Ø£Ø¢Ø§]")
    _TAH_RE = re.compile(r"Ø©")
    _YA_RE = re.compile(r"[ÙŠÙ‰]")

    # Ø£ÙØ¹Ø§Ù„/Ù…Ø­Ø«Ù‘Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
    _ACTION_TRIGGERS = (
        "Ù†ÙÙ‘Ø°", "Ù†ÙØ°", "Ø«Ø¨Ù‘Øª", "Ø«Ø¨Øª", "Ø§Ø¨Ø¯Ø£", "Ø§Ø¨Ø¯Ø§", "Ø§Ø¹Ù…Ù„",
        "Ø£Ù†Ø´Ø¦", "Ø§Ù†Ø´Ø¦", "Ø§Ø¨Ù†Ù", "Ø§Ø¨Ù†ÙŠ", "Ø§Ø¨Ù†", "Ø§Ø­ØµÙ„", "Ø®Ø·ÙˆØ§Øª", "Ø·Ø±ÙŠÙ‚Ø©", "ÙƒÙŠÙ"
    )

    # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¬ÙÙ…Ù„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©
    _STEP_HINTS = (
        "Ø®Ø·ÙˆØ©", "Ø§Ù„Ø®Ø·ÙˆØ©", "Ø§Ø¨Ø¯Ø£", "Ù‚Ù… Ø¨", "Ù†ÙÙ‘Ø°", "Ù†ÙØ°", "Ø«Ø¨Ù‘Øª", "Ø«Ø¨Øª",
        "Ø¥Ù†Ø´Ø¦", "Ø£Ù†Ø´Ø¦", "Ø§Ù†Ø´Ø¦", "Ø§Ø¨Ù†Ù", "Ø§Ø¨Ù†ÙŠ", "ÙØ¹Ù‘Ù„", "ÙØ¹Ù„", "Ø£Ø¶Ù", "Ø§Ø¶Ù",
        "Ø­Ù…Ù‘Ù„", "Ø­Ù…Ù„", "Ø´ØºÙ‘Ù„", "Ø´ØºÙ„", "Ø§Ø³ØªØ®Ø¯Ù…", "Ù‡ÙŠÙ‘Ø¦", "Ù‡ÙŠØ¦", "Ø§Ø°Ù‡Ø¨ Ø¥Ù„Ù‰", "Ø§ÙØªØ­", "Ø§Ø®ØªÙØ±", "Ø§Ø®ØªØ±"
    )

    def _normalize(self, s: str) -> str:
        s = self._DIAC_RE.sub("", s or "")
        s = self._ALEF_RE.sub("Ø§", s)
        s = self._TAH_RE.sub("Ù‡", s)
        s = self._YA_RE.sub("ÙŠ", s)
        return s

    def _clean_text(self, s: str) -> str:
        s = self._URL_RE.sub("", s or "")
        s = self._WS_RE.sub(" ", s)
        s = self._NL_RE.sub("\n\n", s)
        return s.strip()

    def _domain(self, url: str) -> str:
        try:
            netloc = urlparse(url).netloc
            return netloc.lstrip("www.")
        except Exception:
            return url

    def _extract_urls_from_snippets(self, web_snippets: List[str]) -> List[str]:
        urls: List[str] = []
        for sn in web_snippets or []:
            m = self._BOX_URL_RE.search(sn)
            if m:
                urls.append(m.group(1))
        seen, out = set(), []
        for u in urls:
            if u not in seen:
                seen.add(u); out.append(u)
        return out

    # ---------- ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø¬Ù…Ù„ Ù…Ø±Ø´Ù‘Ø­Ø© ----------
    def _sentences_from_sources(self, wiki: str, web_snippets: List[str], context: str) -> Tuple[List[str], List[str], List[str]]:
        def split_clean(text: str) -> List[str]:
            text = self._clean_text(text)
            parts = [p.strip() for p in self._SPLIT_RE.split(text) if p.strip()]
            return [p for p in parts if len(p) >= 20]

        wiki_sents = split_clean(wiki) if wiki else []
        web_sents = []
        for sn in web_snippets or []:
            s = sn.lstrip("- ").split("[http", 1)[0].strip()
            web_sents.extend(split_clean(s))
        ctx_sents = split_clean(context) if context else []
        return wiki_sents, web_sents, ctx_sents

    # ---------- ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„ Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø© (TF-IDF) + Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± ----------
    def _rank_and_dedup(self, query: str, candidates: List[str], top_n: int = 12, sim_th: float = 0.8) -> List[str]:
        if not candidates:
            return []
        norm_query = self._normalize(query)
        norm_cands = [self._normalize(c) for c in candidates]
        vec = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=0.95)
        X = vec.fit_transform([norm_query] + norm_cands)
        qv, cv = X[0:1], X[1:]
        sims = cosine_similarity(qv, cv).ravel()
        order = sims.argsort()[::-1]

        selected, selected_vecs = [], []
        for idx in order:
            sent = candidates[idx]
            v = cv[idx:idx + 1]
            if selected_vecs:
                dup = cosine_similarity(v, selected_vecs).max()
                if dup >= sim_th:
                    continue
            selected.append(sent)
            selected_vecs.append(v)
            if len(selected) >= top_n:
                break
        return selected

    # ---------- ÙƒØ´Ù Ù†ÙŠØ© "Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©" ----------
    def _wants_steps(self, query: str) -> bool:
        q = self._normalize(query)
        return any(t in q for t in self._ACTION_TRIGGERS)

    # ---------- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…Ù„ Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ© ÙƒØ®Ø·ÙˆØ§Øª ----------
    def _extract_steps(self, query: str, web_top: List[str], ctx_top: List[str], wiki_top: List[str], max_steps: int = 6) -> List[str]:
        # Ù†Ø¹Ø·ÙŠ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©
        def is_step_like(s: str) -> bool:
            s_norm = self._normalize(s)
            return any(h in s_norm for h in self._STEP_HINTS) or s_norm.startswith(("Ø§Ø¨Ø¯Ø£", "Ø§Ø¨Ø¯Ø§", "Ù‚Ù…", "Ø§ÙØªØ­", "Ø§Ø®ØªÙØ±", "Ø§Ø®ØªØ±", "Ø§Ø³ØªØ®Ø¯Ù…"))

        pools = [web_top, ctx_top, wiki_top]  # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©: Ø§Ù„ÙˆÙŠØ¨ Ø«Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø«Ù… ÙˆÙŠÙƒÙŠ
        raw: List[str] = []
        for pool in pools:
            for s in pool:
                if is_step_like(s):
                    raw.append(s)
        # Ø¥Ù† Ù„Ù… Ù†Ø¬Ø¯ Ø¬Ù…Ù„Ù‹Ø§ ÙˆØ§Ø¶Ø­Ø©ØŒ Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ Ø¬Ù…Ù„ Ù…ØªÙ†ÙˆÙ‘Ø¹Ø© ÙƒØ¨Ø¯Ø§Ø¦Ù„ Ø®Ø·ÙˆØ§Øª
        if not raw:
            for pool in pools:
                raw.extend(pool[:3])
                if len(raw) >= max_steps:
                    break

        # ØªØ±ØªÙŠØ¨ Ø¥Ø¶Ø§ÙÙŠ Ø¨Ø§Ù„ØµÙ„ÙØ© Ù„Ù„Ø³Ø¤Ø§Ù„ (Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø®ÙÙŠÙ)
        ranked = self._rank_and_dedup(query, raw, top_n=max_steps, sim_th=0.85)
        # ØªÙ†Ø¸ÙŠÙ ÙˆØªØ¹Ø¯Ø§Ø¯
        steps = []
        for i, s in enumerate(ranked[:max_steps], 1):
            s = s.rstrip(" .ØŒ")
            steps.append(f"{i}) {s}")
        return steps

    # ---------- Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… ----------
    def _build_sections(self, query: str, wiki_top: List[str], web_top: List[str], ctx_top: List[str], include_steps: bool) -> Tuple[str, List[str], List[str], List[str]]:
        """
        ÙŠÙØ±Ø¬Ø¹: (Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©ØŒ Ø£Ù‡Ù… Ø§Ù„Ù†Ù‚Ø§Ø·ØŒ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§ØªØŒ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©)
        """
        # Ø§Ù„Ø®Ù„Ø§ØµØ©
        summary = ""
        for pool in (wiki_top, web_top, ctx_top):
            if pool:
                summary = pool[0]; break
        if not summary:
            summary = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø®Ù„Ø§ØµØ© Ù…ÙˆØ«ÙˆÙ‚Ø© Ù„Ù„Ø³Ø¤Ø§Ù„."

        # Ø£Ù‡Ù… Ø§Ù„Ù†Ù‚Ø§Ø·
        bullets = []
        for src in (wiki_top, web_top, ctx_top):
            for s in src[:4]:
                bullets.append(s)
                if len(bullets) >= 7:
                    break
            if len(bullets) >= 7:
                break

        # ØªØ­Ø°ÙŠØ±Ø§Øª
        warns = []
        low_evidence = (len(wiki_top) + len(web_top) + len(ctx_top)) < 3
        if low_evidence:
            warns.append("Ù‚Ø¯ ØªÙƒÙˆÙ† Ø¨Ø¹Ø¶ Ø§Ù„ØªÙØ§ØµÙŠÙ„ ØºÙŠØ± Ù…ÙƒØªÙ…Ù„Ø© Ø¨Ø³Ø¨Ø¨ Ù…Ø­Ø¯ÙˆØ¯ÙŠØ© Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªØ§Ø­.")
        if any(k in query for k in ["Ø£ÙØ¶Ù„", "Ø£Ø­Ø³Ù†", "Ø£Ù‚ÙˆÙ‰", "ultimate", "best"]):
            warns.append("Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± 'Ø§Ù„Ø£ÙØ¶Ù„' Ù‚Ø¯ ÙŠØ®ØªÙ„Ù Ø¨Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©.")
        if any(k in query for k in ["Ø³Ø±ÙŠØ¹", "Ø¨Ø³Ø±Ø¹Ø©", "ÙÙˆØ±ÙŠ"]):
            warns.append("Ø§Ù„Ø³Ø±Ø¹Ø© Ù‚Ø¯ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø¬ÙˆØ¯Ø©â€”ÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø²Ù…Ù† ÙˆØ§Ù„Ø¯Ù‚Ø©.")

        # Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ©
        steps: List[str] = []
        if include_steps:
            steps = self._extract_steps(query, web_top, ctx_top, wiki_top, max_steps=6)
            if not steps:
                # Ù‚Ø§Ù„Ø¨ Ø¹Ø§Ù… ÙÙŠ Ø­Ø§Ù„ Ù†Ø¯Ø±Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
                steps = [
                    "1) Ø­Ø¯Ù‘Ø¯ Ø§Ù„Ù‡Ø¯Ù ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨Ø¯Ù‚Ø© (Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª/Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª/Ø§Ù„Ø²Ù…Ù†/Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©).",
                    "2) Ø¬Ù‡Ù‘Ø² Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ØªÙ†ÙÙŠØ°ØŒ ÙˆØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª.",
                    "3) Ø·Ø¨Ù‘Ù‚ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰ (Ø§Ø¨Ø¯Ø£ Ø¨Ø£Ø¨Ø³Ø· Ù†Ø³Ø®Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¹Ù…Ù„).",
                    "4) Ø§Ø®ØªØ¨Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¨ÙƒØ±Ù‹Ø§ØŒ ÙˆØ³Ø¬Ù‘Ù„ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙˆØ§Ù„Ø£Ø®Ø·Ø§Ø¡.",
                    "5) Ø­Ø³Ù‘Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡/Ø§Ù„Ø¯Ù‚Ø© ØªØ¯Ø±Ù‘Ø¬ÙŠÙ‹Ø§ ÙˆØ£Ø¹Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±.",
                    "6) ÙˆØ«Ù‘Ù‚ Ù…Ø§ ØªÙ…Ù‘ ÙˆØªØ£ÙƒØ¯ Ù…Ù† Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ§Ù„Ù†Ø´Ø±."
                ]

        return summary, bullets, warns, steps

    # ---------- Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù€ main.py) ----------
    def answer(
        self,
        *,
        query: str,
        context: str,
        intent: str,
        sentiment: str,
        web_snippets: List[str],
        wiki: str,
    ) -> str:
        """
        ØªÙˆÙ„ÙŠØ¯ Ù…Ø­Ù„ÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„:
        1) ØªØ­ÙˆÙŠÙ„ (ÙˆÙŠÙƒÙŠ/Ø§Ù„ÙˆÙŠØ¨/Ø§Ù„Ø³ÙŠØ§Ù‚) Ø¥Ù„Ù‰ Ø¬Ù…Ù„ Ù…Ø±Ø´Ù‘Ø­Ø©.
        2) ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„ Ø­Ø³Ø¨ ØµÙ„ØªÙ‡Ø§ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ø¨Ø± TF-IDF.
        3) Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… (Ø®Ù„Ø§ØµØ©/Ù†Ù‚Ø§Ø·/ØªØ­Ø°ÙŠØ±Ø§Øª/Ø®Ø·ÙˆØ§Øª).
        4) Ø¥Ø¶Ø§ÙØ© Ø³Ø·Ø± Ù…ØµØ§Ø¯Ø± Ù†Ø¸ÙŠÙØ© Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· (Ø¥Ù† ÙˆØ¬Ø¯Øª).
        """
        web_snippets = web_snippets or []

        # Ù…ØµØ§Ø¯Ø± Ù†Ø¸ÙŠÙØ©
        urls = self._extract_urls_from_snippets(web_snippets)[:6]
        domains_line = ""
        if urls:
            domains = [self._domain(u) for u in urls]
            domains_line = " â€” Ø§Ù„Ù…ØµØ§Ø¯Ø±: " + " | ".join(domains)

        # Ø¬Ù…Ù„ Ù…Ø±Ø´Ù‘Ø­Ø©
        wiki_s, web_s, ctx_s = self._sentences_from_sources(wiki, web_snippets, context)

        # ØªØ±ØªÙŠØ¨ ÙˆØ¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø±
        wiki_top = self._rank_and_dedup(query, wiki_s, top_n=4, sim_th=0.82)
        web_top  = self._rank_and_dedup(query, web_s,  top_n=6, sim_th=0.82)
        ctx_top  = self._rank_and_dedup(query, ctx_s,  top_n=6, sim_th=0.82)

        # Ù‡Ù„ Ù†Ø¹Ø±Ø¶ "Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ©"ØŸ
        include_steps = self._wants_steps(query)

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
        summary, bullets, warns, steps = self._build_sections(query, wiki_top, web_top, ctx_top, include_steps)

        # ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        parts: List[str] = []
        parts.append(f"â“ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}")
        parts.append("ğŸ“˜ Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©:\n" + summary)

        if bullets:
            parts.append("ğŸ”¹ Ø£Ù‡Ù… Ø§Ù„Ù†Ù‚Ø§Ø·:\n" + "\n".join(f"- {b}" for b in bullets))

        if steps:
            parts.append("ğŸ›  Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ©:\n" + "\n".join(steps))

        if warns:
            parts.append("âš ï¸ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª:\n" + "\n".join(f"- {w}" for w in warns))

        meta = f"ğŸ§  Ø§Ù„Ù†ÙŠØ©: {intent} â€” Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {sentiment}"
        if domains_line:
            meta += domains_line
        parts.append(meta)

        text = "\n\n".join(parts)
        text = self._clean_text(text)
        return self._wrap_style(text)
